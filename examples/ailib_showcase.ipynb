{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AILib: Comprehensive Feature Showcase\n",
    "\n",
    "This notebook demonstrates all the features of AILib, a simple and intuitive Python SDK for building LLM-powered applications.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Basic LLM Completions](#basic-completions)\n",
    "3. [Prompt Templates](#prompt-templates)\n",
    "4. [Building Conversations with Prompt Builder](#prompt-builder)\n",
    "5. [Session Management](#session-management)\n",
    "6. [Chains: Sequential Operations](#chains)\n",
    "7. [Tools and Decorators](#tools)\n",
    "8. [Agents: Autonomous Problem Solving](#agents)\n",
    "9. [Advanced Features](#advanced)\n",
    "10. [Real-World Examples](#examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation <a id='setup'></a>\n",
    "\n",
    "First, let's set up the environment and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install AILib (if not already installed)\n",
    "# !pip install -e ..\n",
    "\n",
    "# Set up environment variable for OpenAI API key\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all AILib modules\n",
    "from ailib import (\n",
    "    # Core components\n",
    "    OpenAIClient,\n",
    "    Message,\n",
    "    Role,\n",
    "    Prompt,\n",
    "    PromptTemplate,\n",
    "    Session,\n",
    "    \n",
    "    # Chains\n",
    "    Chain,\n",
    "    \n",
    "    # Agents and tools\n",
    "    Agent,\n",
    "    Tool,\n",
    "    ToolRegistry,\n",
    "    tool,\n",
    ")\n",
    "\n",
    "# Import helpers\n",
    "from ailib.core.prompt import create_react_prompt, create_few_shot_prompt\n",
    "\n",
    "print(\"AILib imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic LLM Completions <a id='basic-completions'></a>\n",
    "\n",
    "Let's start with the simplest use case: getting a completion from an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client\n",
    "client = OpenAIClient(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create a simple message\n",
    "messages = [\n",
    "    Message(role=Role.SYSTEM, content=\"You are a helpful assistant.\"),\n",
    "    Message(role=Role.USER, content=\"What is the capital of France?\")\n",
    "]\n",
    "\n",
    "# Get completion\n",
    "response = client.complete(messages)\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"\\nModel used: {response.model}\")\n",
    "print(f\"Tokens used: {response.usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Responses\n",
    "\n",
    "AILib supports streaming for real-time output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream a response token by token\n",
    "messages = [\n",
    "    Message(role=Role.USER, content=\"Write a haiku about programming\")\n",
    "]\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "for token in client.stream(messages):\n",
    "    print(token, end=\"\", flush=True)\n",
    "print(\"\\n\\nStreaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt Templates <a id='prompt-templates'></a>\n",
    "\n",
    "Prompt templates allow you to create reusable prompts with variable substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple template\n",
    "template = PromptTemplate(\n",
    "    \"Translate the following {source_lang} text to {target_lang}: '{text}'\"\n",
    ")\n",
    "\n",
    "# Show template variables\n",
    "print(f\"Template variables: {template.variables}\")\n",
    "\n",
    "# Format the template\n",
    "formatted = template.format(\n",
    "    source_lang=\"English\",\n",
    "    target_lang=\"French\",\n",
    "    text=\"Hello, world!\"\n",
    ")\n",
    "print(f\"\\nFormatted prompt: {formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Templates\n",
    "\n",
    "You can create partial templates by pre-filling some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a partial template with target language pre-filled\n",
    "spanish_translator = template.partial(target_lang=\"Spanish\")\n",
    "\n",
    "print(f\"Remaining variables: {spanish_translator.variables}\")\n",
    "\n",
    "# Use the partial template\n",
    "translations = [\n",
    "    spanish_translator.format(source_lang=\"English\", text=\"Good morning\"),\n",
    "    spanish_translator.format(source_lang=\"French\", text=\"Bonjour\")\n",
    "]\n",
    "\n",
    "for prompt in translations:\n",
    "    print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Templates with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a message from template and get completion\n",
    "message = spanish_translator.create_message(\n",
    "    source_lang=\"English\", \n",
    "    text=\"How are you?\"\n",
    ")\n",
    "\n",
    "response = client.complete([message])\n",
    "print(f\"Translation: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building Conversations with Prompt Builder <a id='prompt-builder'></a>\n",
    "\n",
    "The Prompt builder provides a fluent API for constructing multi-message conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conversation using the fluent API\n",
    "prompt = Prompt()\n",
    "prompt.add_system(\"You are a Python programming tutor.\")\n",
    "prompt.add_user(\"What is a list comprehension?\")\n",
    "prompt.add_assistant(\"A list comprehension is a concise way to create lists in Python...\")\n",
    "prompt.add_user(\"Can you show me an example?\")\n",
    "\n",
    "# Get the messages\n",
    "messages = prompt.build()\n",
    "\n",
    "# Display the conversation\n",
    "for i, msg in enumerate(messages):\n",
    "    print(f\"{i+1}. [{msg.role.value}]: {msg.content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Templates in Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a conversation with templates\n",
    "code_review_prompt = Prompt()\n",
    "code_review_prompt.add_system(\"You are a code reviewer specializing in {language}.\")\n",
    "code_review_prompt.add_template(\n",
    "    \"Review this {language} code for best practices:\\n```{language}\\n{code}\\n```\",\n",
    "    role=Role.USER,\n",
    "    language=\"Python\",\n",
    "    code=\"def add(a,b): return a+b\"\n",
    ")\n",
    "\n",
    "# Get completion\n",
    "messages = code_review_prompt.build()\n",
    "response = client.complete(messages)\n",
    "print(\"Code Review:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Session Management <a id='session-management'></a>\n",
    "\n",
    "Sessions help maintain conversation state and memory across interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new session\n",
    "session = Session()\n",
    "\n",
    "print(f\"Session ID: {session.session_id}\")\n",
    "print(f\"Created at: {session.created_at}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add messages to session\n",
    "session.add_system_message(\"You are a helpful math tutor.\")\n",
    "session.add_user_message(\"What is the Pythagorean theorem?\")\n",
    "\n",
    "# Get response from LLM\n",
    "response = client.complete(session.get_messages())\n",
    "session.add_assistant_message(response.content)\n",
    "\n",
    "print(\"First interaction:\")\n",
    "print(response.content[:200] + \"...\")\n",
    "\n",
    "# Continue the conversation\n",
    "session.add_user_message(\"Can you give me an example with numbers?\")\n",
    "response = client.complete(session.get_messages())\n",
    "session.add_assistant_message(response.content)\n",
    "\n",
    "print(\"\\nSecond interaction (with context):\")\n",
    "print(response.content[:200] + \"...\")\n",
    "\n",
    "print(f\"\\nTotal messages in session: {len(session)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Memory Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store information in session memory\n",
    "session.set_memory(\"user_name\", \"Alice\")\n",
    "session.set_memory(\"topic\", \"Pythagorean theorem\")\n",
    "session.set_memory(\"skill_level\", \"beginner\")\n",
    "\n",
    "# Retrieve memory\n",
    "print(f\"User: {session.get_memory('user_name')}\")\n",
    "print(f\"Topic: {session.get_memory('topic')}\")\n",
    "print(f\"Level: {session.get_memory('skill_level')}\")\n",
    "\n",
    "# Update multiple values\n",
    "session.update_memory({\n",
    "    \"examples_given\": 2,\n",
    "    \"last_question\": \"Pythagorean theorem example\"\n",
    "})\n",
    "\n",
    "print(f\"\\nAll memory: {session._memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert session to dict (for saving)\n",
    "session_data = session.to_dict()\n",
    "print(f\"Session data keys: {list(session_data.keys())}\")\n",
    "\n",
    "# Restore session from dict\n",
    "restored_session = Session.from_dict(session_data)\n",
    "print(f\"\\nRestored session ID: {restored_session.session_id}\")\n",
    "print(f\"Restored messages: {len(restored_session)}\")\n",
    "print(f\"Restored memory: {restored_session.get_memory('user_name')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Chains: Sequential Operations <a id='chains'></a>\n",
    "\n",
    "Chains allow you to execute multiple prompts sequentially, with each step building on the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple chain\n",
    "chain = Chain(client)\n",
    "chain.add_system(\"You are a helpful assistant.\")\n",
    "chain.add_user(\"What is the capital of France?\")\n",
    "\n",
    "# Run the chain\n",
    "result = chain.run()\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Step Chains with Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multi-step chain where each step uses previous results\n",
    "story_chain = (\n",
    "    Chain(client)\n",
    "    .add_system(\"You are a creative storyteller.\")\n",
    "    .add_user(\n",
    "        \"Generate a random character name for a fantasy story\",\n",
    "        name=\"character_name\"\n",
    "    )\n",
    "    .add_user(\n",
    "        \"Create a one-sentence backstory for {character_name}\",\n",
    "        name=\"backstory\"\n",
    "    )\n",
    "    .add_user(\n",
    "        \"Write a short adventure scene featuring {character_name}. Their backstory: {backstory}\",\n",
    "        name=\"scene\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Enable verbose mode to see each step\n",
    "story_chain.verbose(True)\n",
    "\n",
    "# Run the chain\n",
    "final_scene = story_chain.run()\n",
    "print(\"\\n=== Final Scene ===\")\n",
    "print(final_scene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains with Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define processor functions\n",
    "def extract_json(text: str) -> dict:\n",
    "    \"\"\"Extract JSON from LLM response.\"\"\"\n",
    "    # Find JSON in the response\n",
    "    import re\n",
    "    json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "    if json_match:\n",
    "        return json.loads(json_match.group())\n",
    "    return {}\n",
    "\n",
    "def format_product(data: dict) -> str:\n",
    "    \"\"\"Format product data nicely.\"\"\"\n",
    "    return f\"**{data.get('name', 'Unknown')}** - ${data.get('price', 0):.2f}\\n{data.get('description', '')}\"\n",
    "\n",
    "# Create a chain with processors\n",
    "product_chain = (\n",
    "    Chain(client)\n",
    "    .add_system(\"You are a product data generator.\")\n",
    "    .add_user(\n",
    "        \"Generate a random product as JSON with fields: name, price, description\",\n",
    "        processor=extract_json,\n",
    "        name=\"product_data\"\n",
    "    )\n",
    "    .add_user(\n",
    "        \"The product data is: {product_data}. Create a marketing tagline for it.\",\n",
    "        name=\"tagline\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "result = product_chain.run()\n",
    "\n",
    "# Access intermediate results\n",
    "print(\"Product Data:\")\n",
    "print(format_product(product_chain._context['product_data']))\n",
    "print(f\"\\nTagline: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tools and Decorators <a id='tools'></a>\n",
    "\n",
    "Tools extend the capabilities of agents by providing functions they can call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools using the @tool decorator\n",
    "@tool\n",
    "def get_weather(city: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Get the current weather for a city.\n",
    "    \n",
    "    Args:\n",
    "        city: Name of the city\n",
    "        unit: Temperature unit (celsius or fahrenheit)\n",
    "    \"\"\"\n",
    "    # Mock implementation\n",
    "    import random\n",
    "    temp = random.randint(15, 30) if unit == \"celsius\" else random.randint(59, 86)\n",
    "    conditions = random.choice([\"sunny\", \"cloudy\", \"partly cloudy\", \"rainy\"])\n",
    "    return f\"The weather in {city} is {conditions} with a temperature of {temp}Â°{unit[0].upper()}\"\n",
    "\n",
    "@tool(name=\"web_search\", description=\"Search the web for information\")\n",
    "def search(query: str, max_results: int = 3) -> str:\n",
    "    \"\"\"Perform a web search.\"\"\"\n",
    "    # Mock implementation\n",
    "    results = [\n",
    "        f\"Result {i+1}: Information about {query}\"\n",
    "        for i in range(max_results)\n",
    "    ]\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> float:\n",
    "    \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "    # Safe evaluation\n",
    "    allowed = {\"__builtins__\": {}}\n",
    "    allowed.update({\n",
    "        \"abs\": abs, \"round\": round, \"min\": min, \"max\": max,\n",
    "        \"sum\": sum, \"pow\": pow\n",
    "    })\n",
    "    try:\n",
    "        return float(eval(expression, allowed))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"Tools defined successfully!\")\n",
    "print(f\"Weather tool: {get_weather._tool.name}\")\n",
    "print(f\"Search tool: {search._tool.name}\")\n",
    "print(f\"Calculator tool: {calculate._tool.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Tool Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool manually\n",
    "def get_time(timezone: str = \"UTC\") -> str:\n",
    "    \"\"\"Get current time in specified timezone.\"\"\"\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    \n",
    "    try:\n",
    "        tz = pytz.timezone(timezone)\n",
    "        time = datetime.now(tz)\n",
    "        return time.strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "    except:\n",
    "        return datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "# Create Tool instance\n",
    "time_tool = Tool(\n",
    "    name=\"get_time\",\n",
    "    description=\"Get current time in any timezone\",\n",
    "    func=get_time\n",
    ")\n",
    "\n",
    "# Test the tool\n",
    "print(\"Manual tool test:\")\n",
    "print(time_tool.execute(timezone=\"US/Pacific\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Registry Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom tool registry\n",
    "my_registry = ToolRegistry()\n",
    "\n",
    "# Register tools\n",
    "my_registry.register(time_tool)\n",
    "\n",
    "# Define and register a tool with custom registry\n",
    "@tool(registry=my_registry)\n",
    "def translate(text: str, target_language: str) -> str:\n",
    "    \"\"\"Translate text to target language.\"\"\"\n",
    "    # Mock implementation\n",
    "    translations = {\n",
    "        \"spanish\": {\"hello\": \"hola\", \"goodbye\": \"adiÃ³s\", \"thank you\": \"gracias\"},\n",
    "        \"french\": {\"hello\": \"bonjour\", \"goodbye\": \"au revoir\", \"thank you\": \"merci\"},\n",
    "        \"german\": {\"hello\": \"hallo\", \"goodbye\": \"auf wiedersehen\", \"thank you\": \"danke\"}\n",
    "    }\n",
    "    \n",
    "    lang = target_language.lower()\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    if lang in translations and text_lower in translations[lang]:\n",
    "        return translations[lang][text_lower]\n",
    "    return f\"[Translation of '{text}' to {target_language}]\"\n",
    "\n",
    "# List tools in registry\n",
    "print(f\"Tools in registry: {my_registry.list_tools()}\")\n",
    "\n",
    "# Get OpenAI function definitions\n",
    "functions = my_registry.to_openai_functions()\n",
    "print(f\"\\nOpenAI function format:\")\n",
    "for func in functions:\n",
    "    print(f\"- {func['name']}: {func['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Agents: Autonomous Problem Solving <a id='agents'></a>\n",
    "\n",
    "Agents can autonomously decide which tools to use to accomplish tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent with tools\n",
    "agent = Agent(\n",
    "    llm=OpenAIClient(model=\"gpt-4\"),\n",
    "    max_steps=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Add tools to the agent\n",
    "agent.with_tools(get_weather, calculate, search)\n",
    "\n",
    "print(f\"Agent created with {len(agent.tool_registry.list_tools())} tools\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Agent Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a simple task\n",
    "result = agent.run(\"What's the weather in Paris?\")\n",
    "print(f\"\\nFinal Answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Multi-Tool Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a task requiring multiple tools\n",
    "complex_task = \"\"\"\n",
    "I'm planning a trip to Tokyo. Can you:\n",
    "1. Check the weather there\n",
    "2. Calculate the cost if flights are $1200 and hotels are $150/night for 5 nights\n",
    "3. Search for top tourist attractions\n",
    "\"\"\"\n",
    "\n",
    "result = agent.run(complex_task)\n",
    "print(f\"\\nFinal Answer:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a specialized math tutor agent\n",
    "@tool\n",
    "def solve_equation(equation: str) -> str:\n",
    "    \"\"\"Solve algebraic equations step by step.\"\"\"\n",
    "    # Mock implementation\n",
    "    steps = [\n",
    "        f\"Given: {equation}\",\n",
    "        \"Step 1: Isolate the variable\",\n",
    "        \"Step 2: Simplify both sides\",\n",
    "        \"Step 3: Solve for x\",\n",
    "        \"Solution: x = 5\"\n",
    "    ]\n",
    "    return \"\\n\".join(steps)\n",
    "\n",
    "@tool\n",
    "def explain_concept(concept: str) -> str:\n",
    "    \"\"\"Explain a mathematical concept.\"\"\"\n",
    "    explanations = {\n",
    "        \"derivative\": \"The derivative measures the rate of change of a function.\",\n",
    "        \"integral\": \"The integral represents the area under a curve.\",\n",
    "        \"limit\": \"A limit describes the value a function approaches as input approaches a point.\"\n",
    "    }\n",
    "    return explanations.get(concept.lower(), f\"Explanation of {concept}...\")\n",
    "\n",
    "# Create math tutor agent\n",
    "math_tutor = Agent(\n",
    "    llm=client,\n",
    "    max_steps=4\n",
    ")\n",
    "math_tutor.with_tools(calculate, solve_equation, explain_concept)\n",
    "\n",
    "# Test the math tutor\n",
    "question = \"Can you explain what a derivative is and then solve the equation 2x + 5 = 15?\"\n",
    "answer = math_tutor.run(question)\n",
    "print(f\"Math Tutor: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Features <a id='advanced'></a>\n",
    "\n",
    "Let's explore some advanced patterns and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def async_example():\n",
    "    \"\"\"Demonstrate async operations.\"\"\"\n",
    "    # Async completion\n",
    "    messages = [Message(role=Role.USER, content=\"Tell me a joke\")]\n",
    "    response = await client.acomplete(messages)\n",
    "    print(f\"Async response: {response.content}\")\n",
    "    \n",
    "    # Async streaming\n",
    "    print(\"\\nAsync streaming:\")\n",
    "    async for token in client.astream(messages):\n",
    "        print(token, end=\"\", flush=True)\n",
    "    print()\n",
    "    \n",
    "    # Async chain\n",
    "    chain = Chain(client).add_user(\"What is 2+2?\")\n",
    "    result = await chain.arun()\n",
    "    print(f\"\\nAsync chain result: {result}\")\n",
    "\n",
    "# Run async example\n",
    "await async_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom LLM Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailib.core import LLMClient, CompletionResponse\n",
    "from typing import List, Iterator, AsyncIterator, Optional, Any, Dict\n",
    "\n",
    "class MockLLMClient(LLMClient):\n",
    "    \"\"\"A mock LLM client for testing.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"mock-model\"):\n",
    "        super().__init__(model)\n",
    "        self.responses = {\n",
    "            \"greeting\": \"Hello! How can I help you today?\",\n",
    "            \"math\": \"The answer is 42.\",\n",
    "            \"default\": \"I understand your question.\"\n",
    "        }\n",
    "    \n",
    "    def complete(self, messages: List[Message], **kwargs) -> CompletionResponse:\n",
    "        # Simple keyword matching\n",
    "        last_message = messages[-1].content.lower()\n",
    "        \n",
    "        if \"hello\" in last_message or \"hi\" in last_message:\n",
    "            response = self.responses[\"greeting\"]\n",
    "        elif \"math\" in last_message or \"calculate\" in last_message:\n",
    "            response = self.responses[\"math\"]\n",
    "        else:\n",
    "            response = self.responses[\"default\"]\n",
    "            \n",
    "        return CompletionResponse(\n",
    "            content=response,\n",
    "            model=self.model,\n",
    "            usage={\"prompt_tokens\": 10, \"completion_tokens\": 5, \"total_tokens\": 15}\n",
    "        )\n",
    "    \n",
    "    async def acomplete(self, messages: List[Message], **kwargs) -> CompletionResponse:\n",
    "        return self.complete(messages, **kwargs)\n",
    "    \n",
    "    def stream(self, messages: List[Message], **kwargs) -> Iterator[str]:\n",
    "        response = self.complete(messages, **kwargs)\n",
    "        for char in response.content:\n",
    "            yield char\n",
    "    \n",
    "    async def astream(self, messages: List[Message], **kwargs) -> AsyncIterator[str]:\n",
    "        response = self.complete(messages, **kwargs)\n",
    "        for char in response.content:\n",
    "            yield char\n",
    "\n",
    "# Test the mock client\n",
    "mock_client = MockLLMClient()\n",
    "response = mock_client.complete([Message(role=Role.USER, content=\"Hello there!\")])\n",
    "print(f\"Mock response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct prompt helper\n",
    "tools = [\"search\", \"calculator\", \"weather\"]\n",
    "messages = create_react_prompt(\n",
    "    question=\"What's the population of Paris multiplied by 2?\",\n",
    "    tools=tools\n",
    ")\n",
    "\n",
    "print(\"ReAct Prompt:\")\n",
    "for msg in messages:\n",
    "    print(f\"\\n[{msg.role.value}]:\\n{msg.content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot learning helper\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"big\", \"output\": \"small\"},\n",
    "    {\"input\": \"hot\", \"output\": \"cold\"}\n",
    "]\n",
    "\n",
    "messages = create_few_shot_prompt(\n",
    "    instruction=\"Find the opposite word\",\n",
    "    examples=examples,\n",
    "    query=\"fast\"\n",
    ")\n",
    "\n",
    "print(\"Few-shot Prompt:\")\n",
    "for i, msg in enumerate(messages):\n",
    "    print(f\"{i+1}. [{msg.role.value}]: {msg.content}\")\n",
    "\n",
    "# Get completion\n",
    "response = client.complete(messages)\n",
    "print(f\"\\nAnswer: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Real-World Examples <a id='examples'></a>\n",
    "\n",
    "Let's build some practical applications using AILib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Code Documentation Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc_generator():\n",
    "    \"\"\"Build a code documentation generator.\"\"\"\n",
    "    \n",
    "    # Create a documentation template\n",
    "    doc_template = PromptTemplate(\"\"\"\n",
    "Generate comprehensive documentation for this {language} code:\n",
    "\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Include:\n",
    "1. Brief description\n",
    "2. Parameters/Arguments\n",
    "3. Return value\n",
    "4. Example usage\n",
    "\"\"\")\n",
    "    \n",
    "    # Create a chain for generating docs\n",
    "    doc_chain = (\n",
    "        Chain(client)\n",
    "        .add_system(\"You are a technical documentation expert.\")\n",
    "        .add_template(doc_template, language=\"Python\", code=\"{code}\")\n",
    "    )\n",
    "    \n",
    "    return doc_chain\n",
    "\n",
    "# Test the doc generator\n",
    "doc_gen = build_doc_generator()\n",
    "\n",
    "sample_code = \"\"\"\n",
    "def fibonacci(n: int) -> int:\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    return fibonacci(n-1) + fibonacci(n-2)\n",
    "\"\"\"\n",
    "\n",
    "documentation = doc_gen.run(code=sample_code)\n",
    "print(\"Generated Documentation:\")\n",
    "print(documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Interactive Tutoring System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TutoringSystem:\n",
    "    \"\"\"An interactive tutoring system using AILib.\"\"\"\n",
    "    \n",
    "    def __init__(self, subject: str):\n",
    "        self.subject = subject\n",
    "        self.session = Session()\n",
    "        self.client = OpenAIClient(model=\"gpt-3.5-turbo\")\n",
    "        \n",
    "        # Initialize session\n",
    "        self.session.add_system_message(\n",
    "            f\"\"\"You are an expert {subject} tutor. \n",
    "            Adapt your explanations to the student's level.\n",
    "            Ask follow-up questions to ensure understanding.\"\"\"\n",
    "        )\n",
    "        \n",
    "        # Track progress\n",
    "        self.session.set_memory(\"topics_covered\", [])\n",
    "        self.session.set_memory(\"student_level\", \"unknown\")\n",
    "    \n",
    "    def ask_question(self, question: str) -> str:\n",
    "        \"\"\"Process a student question.\"\"\"\n",
    "        # Add question to session\n",
    "        self.session.add_user_message(question)\n",
    "        \n",
    "        # Get response\n",
    "        response = self.client.complete(self.session.get_messages())\n",
    "        self.session.add_assistant_message(response.content)\n",
    "        \n",
    "        # Update topics covered\n",
    "        topics = self.session.get_memory(\"topics_covered\")\n",
    "        if question.lower() not in str(topics).lower():\n",
    "            topics.append(question[:50])  # Store first 50 chars\n",
    "            self.session.set_memory(\"topics_covered\", topics)\n",
    "        \n",
    "        return response.content\n",
    "    \n",
    "    def get_summary(self) -> str:\n",
    "        \"\"\"Get a summary of the tutoring session.\"\"\"\n",
    "        topics = self.session.get_memory(\"topics_covered\")\n",
    "        \n",
    "        summary_prompt = f\"\"\"\n",
    "        Based on our conversation, provide a brief summary of:\n",
    "        1. Topics we covered: {topics}\n",
    "        2. Key concepts explained\n",
    "        3. Suggested next steps for the student\n",
    "        \"\"\"\n",
    "        \n",
    "        self.session.add_user_message(summary_prompt)\n",
    "        response = self.client.complete(self.session.get_messages())\n",
    "        \n",
    "        return response.content\n",
    "\n",
    "# Create a math tutor\n",
    "tutor = TutoringSystem(\"Mathematics\")\n",
    "\n",
    "# Simulate a tutoring session\n",
    "print(\"=== Math Tutoring Session ===\")\n",
    "print()\n",
    "\n",
    "response1 = tutor.ask_question(\"What is calculus?\")\n",
    "print(\"Student: What is calculus?\")\n",
    "print(f\"Tutor: {response1[:300]}...\\n\")\n",
    "\n",
    "response2 = tutor.ask_question(\"Can you give me an example of a derivative?\")\n",
    "print(\"Student: Can you give me an example of a derivative?\")\n",
    "print(f\"Tutor: {response2[:300]}...\\n\")\n",
    "\n",
    "# Get session summary\n",
    "print(\"=== Session Summary ===\")\n",
    "summary = tutor.get_summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Research Assistant Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a research assistant with custom tools\n",
    "@tool\n",
    "def analyze_topic(topic: str) -> str:\n",
    "    \"\"\"Analyze a research topic and identify key areas.\"\"\"\n",
    "    return f\"\"\"\n",
    "    Analysis of '{topic}':\n",
    "    - Main concepts: [Key concepts related to {topic}]\n",
    "    - Research areas: [Current research directions]\n",
    "    - Applications: [Practical applications]\n",
    "    - Challenges: [Open problems and challenges]\n",
    "    \"\"\"\n",
    "\n",
    "@tool\n",
    "def find_papers(topic: str, year_start: int = 2020) -> str:\n",
    "    \"\"\"Find relevant research papers on a topic.\"\"\"\n",
    "    # Mock implementation\n",
    "    papers = [\n",
    "        f\"'{topic}: A Comprehensive Survey' (2023) - Smith et al.\",\n",
    "        f\"'Advances in {topic}' (2022) - Johnson & Lee\",\n",
    "        f\"'Future Directions for {topic}' (2024) - Brown et al.\"\n",
    "    ]\n",
    "    return \"Relevant papers:\\n\" + \"\\n\".join(f\"- {p}\" for p in papers)\n",
    "\n",
    "@tool\n",
    "def summarize_findings(papers: str, focus: str = \"general\") -> str:\n",
    "    \"\"\"Summarize research findings.\"\"\"\n",
    "    return f\"\"\"\n",
    "    Summary of findings (focus: {focus}):\n",
    "    1. Current state of the field shows significant progress\n",
    "    2. Key innovations include new methodologies and applications\n",
    "    3. Future work should address scalability and practical deployment\n",
    "    \"\"\"\n",
    "\n",
    "# Create research assistant\n",
    "research_assistant = Agent(\n",
    "    llm=OpenAIClient(model=\"gpt-4\"),\n",
    "    max_steps=6\n",
    ")\n",
    "research_assistant.with_tools(analyze_topic, find_papers, summarize_findings, search)\n",
    "\n",
    "# Conduct research\n",
    "research_query = \"\"\"\n",
    "I need to research 'quantum computing applications in cryptography'. \n",
    "Please analyze the topic, find relevant papers from 2022 onwards, \n",
    "and summarize the key findings focusing on practical applications.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Research Assistant ===\")\n",
    "research_result = research_assistant.run(research_query)\n",
    "print(f\"\\nResearch Summary:\\n{research_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the comprehensive features of AILib:\n",
    "\n",
    "1. **Simple API**: Intuitive interfaces for all components\n",
    "2. **Flexibility**: From basic completions to complex agents\n",
    "3. **Extensibility**: Easy to add custom tools and clients\n",
    "4. **Type Safety**: Full type hints throughout\n",
    "5. **Production Ready**: Sessions, error handling, and async support\n",
    "\n",
    "AILib provides a cleaner, more Pythonic alternative to existing frameworks while maintaining powerful capabilities for building LLM applications.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore the remaining low-priority features (validation, safety, tracing)\n",
    "- Build your own custom tools and agents\n",
    "- Integrate with your existing applications\n",
    "- Contribute to the project on GitHub\n",
    "\n",
    "Happy building with AILib! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}