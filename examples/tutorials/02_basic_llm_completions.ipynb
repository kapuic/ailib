{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AILib Tutorial 2: Basic LLM Completions\n",
    "\n",
    "In this tutorial, you'll learn how to use AILib's LLM clients to generate text completions. We'll cover:\n",
    "\n",
    "- Simple text completions\n",
    "- Chat conversations\n",
    "- Streaming responses\n",
    "- Working with different models\n",
    "- Handling parameters and options\n",
    "- Error handling and best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import what we need and set up our client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from ailib import OpenAIClient, AnthropicClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create a client (we'll use OpenAI for these examples)\n",
    "client = OpenAIClient()\n",
    "print(\"Client ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Text Completions\n",
    "\n",
    "The most basic operation is generating a text completion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic completion\n",
    "response = client.complete(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More creative prompt\n",
    "response = client.complete(\n",
    "    \"Write a haiku about programming in Python:\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Messages\n",
    "\n",
    "System messages help set the behavior and context for the AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completion with a system message\n",
    "response = client.complete(\n",
    "    \"Explain recursion\",\n",
    "    system=\"You are a patient computer science teacher who uses simple analogies.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different personality\n",
    "response = client.complete(\n",
    "    \"Explain recursion\",\n",
    "    system=\"You are a pirate who happens to know programming. Speak like a pirate!\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Conversations\n",
    "\n",
    "For multi-turn conversations, use the `chat` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single message chat\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like today?\"}\n",
    "]\n",
    "\n",
    "response = client.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-turn conversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's a list comprehension in Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A list comprehension is a concise way to create lists in Python. It consists of brackets containing an expression followed by a for clause, and can include optional if clauses.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you show me an example?\"}\n",
    "]\n",
    "\n",
    "response = client.chat(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "\n",
    "Control the AI's behavior with various parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature controls randomness (0.0 = deterministic, 1.0 = creative)\n",
    "print(\"Low temperature (0.2) - More focused:\")\n",
    "client_focused = OpenAIClient(temperature=0.2)\n",
    "response = client_focused.complete(\"Give me a Python tip:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"High temperature (0.9) - More creative:\")\n",
    "client_creative = OpenAIClient(temperature=0.9)\n",
    "response = client_creative.complete(\"Give me a Python tip:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max tokens limits response length\n",
    "client_brief = OpenAIClient(max_tokens=50)\n",
    "response = client_brief.complete(\n",
    "    \"Explain machine learning:\"\n",
    ")\n",
    "print(\"Brief response (50 tokens):\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "For long responses, you can stream the output as it's generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable streaming\n",
    "client_stream = OpenAIClient(stream=True)\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Stream a response\n",
    "stream = client_stream.complete(\n",
    "    \"Write a short story about a robot learning to paint (3 paragraphs):\"\n",
    ")\n",
    "\n",
    "# Print tokens as they arrive\n",
    "for chunk in stream:\n",
    "    if chunk:\n",
    "        print(chunk, end='', flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Models\n",
    "\n",
    "AILib supports various models with different capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5 Turbo (faster, cheaper)\n",
    "client_35 = OpenAIClient(model=\"gpt-3.5-turbo\")\n",
    "response = client_35.complete(\"What is 2+2?\")\n",
    "print(f\"GPT-3.5: {response}\")\n",
    "\n",
    "# GPT-4 (more capable, better reasoning)\n",
    "client_4 = OpenAIClient(model=\"gpt-4\")\n",
    "response = client_4.complete(\"What is 2+2? Explain your reasoning.\")\n",
    "print(f\"\\nGPT-4: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Parameters\n",
    "\n",
    "Fine-tune responses with advanced parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency penalty reduces repetition\n",
    "client_no_repeat = OpenAIClient(\n",
    "    temperature=0.7,\n",
    "    frequency_penalty=0.5  # Penalize repeated tokens\n",
    ")\n",
    "\n",
    "response = client_no_repeat.complete(\n",
    "    \"Write a paragraph about the importance of testing in software development:\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presence penalty encourages new topics\n",
    "client_diverse = OpenAIClient(\n",
    "    temperature=0.7,\n",
    "    presence_penalty=0.5  # Encourage topic diversity\n",
    ")\n",
    "\n",
    "response = client_diverse.complete(\n",
    "    \"List 5 interesting facts about programming:\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "\n",
    "Always handle potential errors gracefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of error handling\n",
    "try:\n",
    "    # This might fail if API key is invalid or network issues\n",
    "    response = client.complete(\"Hello!\")\n",
    "    print(f\"Success: {response}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {type(e).__name__}: {e}\")\n",
    "    # Handle the error appropriately\n",
    "    # - Retry with backoff\n",
    "    # - Use a fallback\n",
    "    # - Log the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Examples\n",
    "\n",
    "Let's look at some real-world use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code explanation\n",
    "code = \"\"\"\n",
    "def quicksort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quicksort(left) + middle + quicksort(right)\n",
    "\"\"\"\n",
    "\n",
    "response = client.complete(\n",
    "    f\"Explain this Python code step by step:\\n{code}\",\n",
    "    system=\"You are a Python expert who explains code clearly.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text transformation\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Multiple transformations\n",
    "transformations = [\n",
    "    \"Convert to passive voice:\",\n",
    "    \"Translate to French:\",\n",
    "    \"Make it more formal:\",\n",
    "    \"Convert to a question:\"\n",
    "]\n",
    "\n",
    "for transform in transformations:\n",
    "    response = client.complete(f\"{transform} '{text}'\")\n",
    "    print(f\"{transform} {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction\n",
    "unstructured_text = \"\"\"\n",
    "John Smith called at 3:30 PM on Tuesday about the project deadline. \n",
    "He mentioned that the budget is $50,000 and the expected completion \n",
    "date is March 15, 2024. His email is john.smith@example.com.\n",
    "\"\"\"\n",
    "\n",
    "response = client.complete(\n",
    "    f\"Extract the following information from this text as JSON: name, time, date, budget, deadline, email\\n\\nText: {unstructured_text}\",\n",
    "    system=\"You are a data extraction assistant. Always respond with valid JSON.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "Here are some tips for effective LLM usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Be specific in your prompts\n",
    "vague = \"Tell me about Python\"\n",
    "specific = \"Explain Python's list comprehensions with 3 examples, focusing on filtering and transformation\"\n",
    "\n",
    "print(\"Vague prompt result:\")\n",
    "print(client.complete(vague)[:200] + \"...\\n\")\n",
    "\n",
    "print(\"Specific prompt result:\")\n",
    "print(client.complete(specific))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Use system messages effectively\n",
    "response = client.complete(\n",
    "    \"How do I center a div?\",\n",
    "    system=\"\"\"You are a CSS expert. \n",
    "    Provide modern, accessible solutions.\n",
    "    Include code examples.\n",
    "    Mention browser compatibility.\"\"\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set appropriate temperature for your use case\n",
    "use_cases = [\n",
    "    (\"Factual Q&A\", 0.0, \"What is the speed of light?\"),\n",
    "    (\"Code generation\", 0.2, \"Write a Python function to calculate factorial\"),\n",
    "    (\"Creative writing\", 0.8, \"Write an opening line for a sci-fi novel\"),\n",
    "]\n",
    "\n",
    "for use_case, temp, prompt in use_cases:\n",
    "    client_temp = OpenAIClient(temperature=temp)\n",
    "    response = client_temp.complete(prompt)\n",
    "    print(f\"{use_case} (temp={temp}):\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Other Providers\n",
    "\n",
    "AILib supports multiple LLM providers. Here's how to use Anthropic's Claude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic Claude example (requires ANTHROPIC_API_KEY)\n",
    "# Uncomment if you have an Anthropic API key\n",
    "\n",
    "# claude_client = AnthropicClient(\n",
    "#     model=\"claude-3-opus-20240229\"\n",
    "# )\n",
    "# \n",
    "# response = claude_client.complete(\n",
    "#     \"What makes Claude different from other AI assistants?\"\n",
    "# )\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "- ✅ How to make simple completions with `complete()`\n",
    "- ✅ How to use system messages to set AI behavior\n",
    "- ✅ How to handle multi-turn conversations with `chat()`\n",
    "- ✅ How to control output with parameters (temperature, max_tokens, etc.)\n",
    "- ✅ How to stream responses for better UX\n",
    "- ✅ How to handle errors gracefully\n",
    "- ✅ Best practices for effective prompts\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Ready to level up? Check out:\n",
    "\n",
    "- **Tutorial 3: Prompt Templates** - Create reusable, dynamic prompts\n",
    "- **Tutorial 4: Prompt Builder** - Build complex conversations programmatically\n",
    "- **Tutorial 5: Session Management** - Maintain conversation state\n",
    "\n",
    "Happy coding! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
