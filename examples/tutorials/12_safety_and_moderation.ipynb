{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AILib Tutorial 12: Safety and Moderation\n",
    "\n",
    "Learn how to build safe and responsible AI applications with AILib's built-in safety features:\n",
    "\n",
    "- Content moderation and filtering\n",
    "- Rate limiting to prevent abuse\n",
    "- Custom safety rules\n",
    "- Integration with OpenAI's moderation API\n",
    "- Safety hooks for LLM clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ailib.safety import (\n",
    "    enable_safety, disable_safety, check_content,\n",
    "    set_rate_limit, check_rate_limit, reset_rate_limit,\n",
    "    with_moderation, add_custom_filter\n",
    ")\n",
    "from ailib import create_agent, create_chain, OpenAIClient\n",
    "import os\n",
    "\n",
    "# Make sure you have your API key\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Content Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enable safety features globally\nenable_safety(\n    block_harmful=True,\n    max_length=1000,\n    blocked_words=[\"violence\", \"hate\", \"self-harm\"]\n)\n\n# Check content directly\nsafe_text = \"Hello, how can I help you today?\"\nis_safe, violations = check_content(safe_text)\nprint(f\"Text: '{safe_text}'\")\nprint(f\"Safe: {is_safe}, Violations: {violations}\")\n\n# Test with problematic content\nlong_text = \"a\" * 2000  # Too long\nis_safe, violations = check_content(long_text)\nprint(f\"\\nLong text safe: {is_safe}\")\nprint(f\"Violations: {violations}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Safety Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom filters for your specific needs\n",
    "import re\n",
    "\n",
    "# Add regex-based filter for phone numbers\n",
    "add_custom_filter(\n",
    "    name=\"phone_filter\",\n",
    "    pattern=r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b',\n",
    "    message=\"Phone numbers are not allowed\"\n",
    ")\n",
    "\n",
    "# Add regex for email addresses\n",
    "add_custom_filter(\n",
    "    name=\"email_filter\",\n",
    "    pattern=r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "    message=\"Email addresses are not allowed\"\n",
    ")\n",
    "\n",
    "# Test custom filters\n",
    "test_texts = [\n",
    "    \"Call me at 555-123-4567\",\n",
    "    \"Email me at user@example.com\",\n",
    "    \"This is a safe message\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    is_safe, violations = check_content(text)\n",
    "    print(f\"\\nText: '{text}'\")\n",
    "    print(f\"Safe: {is_safe}, Violations: {violations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Moderation Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get moderation hooks for OpenAI\n",
    "pre_hook, post_hook = with_moderation(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Use with an LLM client\n",
    "client = OpenAIClient()\n",
    "\n",
    "# Add hooks to client (if supported)\n",
    "# Note: This is a demonstration - actual implementation may vary\n",
    "print(\"Moderation hooks created:\")\n",
    "print(f\"- Pre-hook: Checks input before sending to LLM\")\n",
    "print(f\"- Post-hook: Checks LLM output before returning\")\n",
    "\n",
    "# Example of how it would work:\n",
    "def safe_completion(prompt: str) -> str:\n",
    "    \"\"\"Example of using moderation hooks.\"\"\"\n",
    "    # Pre-check\n",
    "    try:\n",
    "        pre_hook(prompt)\n",
    "    except Exception as e:\n",
    "        return f\"Input blocked: {e}\"\n",
    "    \n",
    "    # Get completion (mock)\n",
    "    response = \"This is a mock response\"\n",
    "    \n",
    "    # Post-check\n",
    "    try:\n",
    "        post_hook(response)\n",
    "    except Exception as e:\n",
    "        return f\"Output blocked: {e}\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test it\n",
    "result = safe_completion(\"Tell me a story\")\n",
    "print(f\"\\nResult: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up rate limiting\n",
    "set_rate_limit(\n",
    "    max_requests=5,\n",
    "    window_seconds=60  # 5 requests per minute\n",
    ")\n",
    "\n",
    "# Simulate requests from different users\n",
    "users = [\"user1\", \"user2\", \"user1\", \"user1\", \"user1\", \"user1\", \"user1\"]\n",
    "\n",
    "for i, user in enumerate(users):\n",
    "    allowed = check_rate_limit(user)\n",
    "    print(f\"Request {i+1} from {user}: {'✅ Allowed' if allowed else '❌ Blocked'}\")\n",
    "\n",
    "# Reset rate limit for a user\n",
    "print(\"\\nResetting rate limit for user1...\")\n",
    "reset_rate_limit(\"user1\")\n",
    "allowed = check_rate_limit(\"user1\")\n",
    "print(f\"After reset: {'✅ Allowed' if allowed else '❌ Blocked'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety with Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a safety-enabled agent\n",
    "from ailib import tool\n",
    "\n",
    "@tool\n",
    "def process_user_data(data: str) -> str:\n",
    "    \"\"\"Process user-provided data.\"\"\"\n",
    "    # Check safety before processing\n",
    "    is_safe, violations = check_content(data)\n",
    "    if not is_safe:\n",
    "        return f\"Cannot process: {violations}\"\n",
    "    return f\"Processed: {data.upper()}\"\n",
    "\n",
    "# Create agent with safety in mind\n",
    "safe_agent = create_agent(\n",
    "    \"safe_assistant\",\n",
    "    tools=[process_user_data],\n",
    "    instructions=\"\"\"You are a safety-conscious assistant.\n",
    "    Always check content safety before processing.\n",
    "    Refuse any requests that might be harmful.\"\"\",\n",
    "    temperature=0.3  # Lower temperature for more consistent behavior\n",
    ")\n",
    "\n",
    "# Test with various inputs\n",
    "test_queries = [\n",
    "    \"Process this data: Hello World\",\n",
    "    \"Process this data: Call 555-1234\",  # Contains phone number\n",
    "    \"What's the weather like?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    \n",
    "    # Check rate limit first\n",
    "    if check_rate_limit(\"demo_user\"):\n",
    "        result = safe_agent.run(query)\n",
    "        print(f\"Result: {result}\")\n",
    "    else:\n",
    "        print(\"Rate limit exceeded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety Configuration Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Different safety configurations for different use cases\n\n# 1. Strict mode for children's applications\ndef configure_child_safety():\n    enable_safety(\n        block_harmful=True,\n        max_length=500,\n        blocked_words=[\n            \"violence\", \"adult\", \"drugs\", \"weapons\",\n            \"horror\", \"profanity\"\n        ]\n    )\n    set_rate_limit(max_requests=10, window_seconds=3600)  # 10 per hour\n    print(\"✅ Child safety mode enabled\")\n\n# 2. Professional mode for business applications  \ndef configure_professional_safety():\n    enable_safety(\n        block_harmful=True,\n        max_length=4000,\n        blocked_words=[\"harassment\", \"discrimination\"]\n    )\n    # Add PII filters\n    add_custom_filter(\"ssn\", r'\\b\\d{3}-\\d{2}-\\d{4}\\b', \"SSN detected\")\n    add_custom_filter(\"credit_card\", r'\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b', \"Credit card detected\")\n    print(\"✅ Professional safety mode enabled\")\n\n# 3. Research mode with minimal restrictions\ndef configure_research_safety():\n    enable_safety(\n        block_harmful=False,  # Don't block, just flag\n        max_length=10000,\n        blocked_words=[]  # No word restrictions\n    )\n    set_rate_limit(max_requests=100, window_seconds=60)  # Higher limits\n    print(\"✅ Research safety mode enabled\")\n\n# Test different modes\nprint(\"Testing different safety configurations:\\n\")\n\nconfigure_child_safety()\nis_safe, _ = check_content(\"This contains mild violence\")\nprint(f\"Child mode - Violence check: {'Blocked' if not is_safe else 'Allowed'}\")\n\nconfigure_research_safety()\nis_safe, _ = check_content(\"This contains mild violence\")\nprint(f\"Research mode - Violence check: {'Blocked' if not is_safe else 'Allowed'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Content Moderation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete moderation pipeline\n",
    "class ModerationPipeline:\n",
    "    def __init__(self, strict_mode=False):\n",
    "        self.strict_mode = strict_mode\n",
    "        self.configure_safety()\n",
    "        \n",
    "    def configure_safety(self):\n",
    "        if self.strict_mode:\n",
    "            configure_child_safety()\n",
    "        else:\n",
    "            configure_professional_safety()\n",
    "    \n",
    "    def process_input(self, user_id: str, text: str) -> dict:\n",
    "        \"\"\"Process user input through safety checks.\"\"\"\n",
    "        result = {\n",
    "            \"user_id\": user_id,\n",
    "            \"input\": text,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"checks\": {}\n",
    "        }\n",
    "        \n",
    "        # Rate limit check\n",
    "        if not check_rate_limit(user_id):\n",
    "            result[\"blocked\"] = True\n",
    "            result[\"reason\"] = \"Rate limit exceeded\"\n",
    "            return result\n",
    "        \n",
    "        # Content safety check\n",
    "        is_safe, violations = check_content(text)\n",
    "        result[\"checks\"][\"content_safety\"] = {\n",
    "            \"passed\": is_safe,\n",
    "            \"violations\": violations\n",
    "        }\n",
    "        \n",
    "        if not is_safe:\n",
    "            result[\"blocked\"] = True\n",
    "            result[\"reason\"] = f\"Content violations: {violations}\"\n",
    "            return result\n",
    "        \n",
    "        # All checks passed\n",
    "        result[\"blocked\"] = False\n",
    "        result[\"processed\"] = True\n",
    "        return result\n",
    "    \n",
    "    def get_safe_response(self, user_id: str, query: str) -> str:\n",
    "        \"\"\"Get LLM response with full safety checks.\"\"\"\n",
    "        # Check input\n",
    "        input_check = self.process_input(user_id, query)\n",
    "        if input_check.get(\"blocked\"):\n",
    "            return f\"Request blocked: {input_check['reason']}\"\n",
    "        \n",
    "        # Process with LLM (mock)\n",
    "        response = f\"Response to: {query[:50]}...\"\n",
    "        \n",
    "        # Check output\n",
    "        output_check = self.process_input(user_id, response)\n",
    "        if output_check.get(\"blocked\"):\n",
    "            return \"Response filtered for safety reasons\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create and test pipeline\n",
    "from datetime import datetime\n",
    "\n",
    "pipeline = ModerationPipeline(strict_mode=False)\n",
    "\n",
    "# Test various scenarios\n",
    "test_cases = [\n",
    "    (\"user1\", \"What's the weather today?\"),\n",
    "    (\"user2\", \"My SSN is 123-45-6789\"),  # PII\n",
    "    (\"user3\", \"Tell me about machine learning\"),\n",
    "    (\"user1\", \"Another question\"),  # Rate limit test\n",
    "]\n",
    "\n",
    "for user_id, query in test_cases:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"User: {user_id}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    result = pipeline.get_safe_response(user_id, query)\n",
    "    print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Safe agent factory\n",
    "def create_safe_agent(name: str, purpose: str, **kwargs):\n",
    "    \"\"\"Create an agent with safety features pre-configured.\"\"\"\n",
    "    \n",
    "    # Set safety instructions\n",
    "    safety_instructions = f\"\"\"\n",
    "You are a safe and responsible AI assistant designed for {purpose}.\n",
    "\n",
    "Safety guidelines:\n",
    "- Never process or output personal information (SSN, credit cards, etc.)\n",
    "- Refuse requests for harmful, illegal, or unethical content\n",
    "- Keep responses appropriate and professional\n",
    "- If unsure about safety, err on the side of caution\n",
    "\"\"\"\n",
    "    \n",
    "    # Merge with user instructions\n",
    "    user_instructions = kwargs.get('instructions', '')\n",
    "    kwargs['instructions'] = safety_instructions + \"\\n\" + user_instructions\n",
    "    \n",
    "    # Set conservative defaults\n",
    "    kwargs.setdefault('temperature', 0.3)\n",
    "    kwargs.setdefault('max_steps', 5)\n",
    "    \n",
    "    return create_agent(name, **kwargs)\n",
    "\n",
    "# Create different types of safe agents\n",
    "customer_service = create_safe_agent(\n",
    "    \"customer_service\",\n",
    "    purpose=\"customer support\",\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "educational = create_safe_agent(\n",
    "    \"tutor\",\n",
    "    purpose=\"educational assistance\",\n",
    "    model=\"gpt-4\"\n",
    ")\n",
    "\n",
    "print(\"✅ Created safe agents for different purposes\")\n",
    "print(f\"- Customer Service: {customer_service.name}\")\n",
    "print(f\"- Educational: {educational.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "AILib's safety features help you build responsible AI applications:\n",
    "\n",
    "- ✅ **Content filtering** - Block harmful or inappropriate content\n",
    "- ✅ **Rate limiting** - Prevent abuse and manage costs\n",
    "- ✅ **Custom filters** - Add domain-specific safety rules\n",
    "- ✅ **Moderation API** - Integrate with OpenAI's moderation\n",
    "- ✅ **Flexible configuration** - Adapt to different use cases\n",
    "\n",
    "Key takeaways:\n",
    "1. Always enable safety features in production\n",
    "2. Configure based on your use case (strict for children, moderate for business)\n",
    "3. Add custom filters for PII and sensitive data\n",
    "4. Implement rate limiting to prevent abuse\n",
    "5. Test safety features thoroughly before deployment\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore [Tracing and Debugging](13_tracing_and_debugging.ipynb)\n",
    "- See [Real-World Examples](10_real_world_examples.ipynb) with safety integrated\n",
    "- Read about safety best practices in production\n",
    "\n",
    "Stay safe! 🛡️"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
