{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# AILib Tutorial 9: Advanced Features\n\nExplore advanced features and patterns in AILib. This tutorial covers:\n\n- Async operations\n- Streaming responses\n- Custom LLM clients\n- Advanced prompt engineering\n- Performance optimization\n- Integration patterns\n- Error handling strategies\n- Pydantic validation for type safety"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from typing import AsyncIterator, Iterator, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "\n",
    "from ailib import OpenAIClient, LLMClient\n",
    "from ailib.prompts import PromptTemplate, PromptBuilder\n",
    "from ailib.chains import Chain\n",
    "from ailib.agents import Agent, Tool, ToolRegistry, tool\n",
    "from ailib import Session\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create clients\n",
    "client = OpenAIClient()\n",
    "streaming_client = OpenAIClient(stream=True)\n",
    "\n",
    "print(\"Ready for advanced features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Operations\n",
    "\n",
    "AILib supports async operations for better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an async-compatible client\n",
    "class AsyncOpenAIClient(OpenAIClient):\n",
    "    \"\"\"OpenAI client with async support.\"\"\"\n",
    "    \n",
    "    async def acompletion(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Async completion.\"\"\"\n",
    "        # In production, use actual async OpenAI client\n",
    "        # This is a simulation\n",
    "        await asyncio.sleep(0.1)  # Simulate API delay\n",
    "        return self.complete(prompt, **kwargs)\n",
    "    \n",
    "    async def achat(self, messages: list, **kwargs) -> str:\n",
    "        \"\"\"Async chat.\"\"\"\n",
    "        await asyncio.sleep(0.1)  # Simulate API delay\n",
    "        return self.chat(messages, **kwargs)\n",
    "\n",
    "# Async batch processing\n",
    "async def process_batch_async(prompts: list[str]) -> list[str]:\n",
    "    \"\"\"Process multiple prompts concurrently.\"\"\"\n",
    "    async_client = AsyncOpenAIClient()\n",
    "    \n",
    "    # Create tasks for all prompts\n",
    "    tasks = [async_client.acompletion(prompt) for prompt in prompts]\n",
    "    \n",
    "    # Wait for all to complete\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test async processing\n",
    "async def test_async():\n",
    "    prompts = [\n",
    "        \"Write a haiku about Python\",\n",
    "        \"Explain quantum computing in one sentence\",\n",
    "        \"What is the meaning of life?\",\n",
    "        \"Describe the color blue to a blind person\"\n",
    "    ]\n",
    "    \n",
    "    # Time sync processing\n",
    "    start_sync = time.time()\n",
    "    sync_results = [client.complete(p) for p in prompts]\n",
    "    sync_time = time.time() - start_sync\n",
    "    \n",
    "    # Time async processing\n",
    "    start_async = time.time()\n",
    "    async_results = await process_batch_async(prompts)\n",
    "    async_time = time.time() - start_async\n",
    "    \n",
    "    print(f\"Sync processing time: {sync_time:.2f}s\")\n",
    "    print(f\"Async processing time: {async_time:.2f}s\")\n",
    "    print(f\"Speedup: {sync_time/async_time:.2f}x\")\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    for i, (prompt, result) in enumerate(zip(prompts, async_results)):\n",
    "        print(f\"\\n{i+1}. {prompt}\")\n",
    "        print(f\"   {result[:100]}...\")\n",
    "\n",
    "# Run async test\n",
    "await test_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "\n",
    "Handle streaming responses for real-time output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming response handler\n",
    "class StreamHandler:\n",
    "    \"\"\"Handle streaming responses with callbacks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def on_token(self, token: str):\n",
    "        \"\"\"Called for each token.\"\"\"\n",
    "        self.tokens.append(token)\n",
    "        print(token, end='', flush=True)\n",
    "    \n",
    "    def on_complete(self):\n",
    "        \"\"\"Called when streaming completes.\"\"\"\n",
    "        self.metadata['total_tokens'] = len(self.tokens)\n",
    "        self.metadata['full_response'] = ''.join(self.tokens)\n",
    "        print(\"\\n\\n[Streaming complete]\")\n",
    "    \n",
    "    def get_response(self) -> str:\n",
    "        \"\"\"Get the full response.\"\"\"\n",
    "        return ''.join(self.tokens)\n",
    "\n",
    "# Stream with progress tracking\n",
    "def stream_with_progress(prompt: str, max_tokens: int = 200):\n",
    "    \"\"\"Stream response with progress tracking.\"\"\"\n",
    "    handler = StreamHandler()\n",
    "    \n",
    "    print(\"Streaming response:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get streaming response\n",
    "    stream = streaming_client.complete(prompt)\n",
    "    \n",
    "    # Process stream\n",
    "    for token in stream:\n",
    "        if token:\n",
    "            handler.on_token(token)\n",
    "    \n",
    "    handler.on_complete()\n",
    "    \n",
    "    # Show metadata\n",
    "    print(f\"\\nTotal tokens: {handler.metadata['total_tokens']}\")\n",
    "    \n",
    "    return handler.get_response()\n",
    "\n",
    "# Test streaming\n",
    "response = stream_with_progress(\n",
    "    \"Write a short story about a robot learning to paint (max 100 words):\"\n",
    ")\n",
    "\n",
    "# Advanced streaming with filtering\n",
    "class FilteredStreamHandler(StreamHandler):\n",
    "    \"\"\"Stream handler with content filtering.\"\"\"\n",
    "    \n",
    "    def __init__(self, filters: list[str] = None):\n",
    "        super().__init__()\n",
    "        self.filters = filters or []\n",
    "        self.filtered_count = 0\n",
    "    \n",
    "    def on_token(self, token: str):\n",
    "        \"\"\"Filter tokens before processing.\"\"\"\n",
    "        # Check filters\n",
    "        for filter_word in self.filters:\n",
    "            if filter_word.lower() in token.lower():\n",
    "                token = \"[FILTERED]\"\n",
    "                self.filtered_count += 1\n",
    "                break\n",
    "        \n",
    "        super().on_token(token)\n",
    "\n",
    "print(\"\\n\\nFiltered streaming example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# This is just an example - in production, use proper content moderation\n",
    "filtered_handler = FilteredStreamHandler(filters=[\"robot\", \"paint\"])\n",
    "# Process with filtering (mock example)\n",
    "print(\"[Filtered streaming would replace sensitive words]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom LLM Clients\n",
    "\n",
    "Create custom LLM clients for specific providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom LLM client example\n",
    "class CustomLLMClient(LLMClient):\n",
    "    \"\"\"Custom LLM client implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint: str, api_key: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.endpoint = endpoint\n",
    "        self.api_key = api_key\n",
    "        self.request_count = 0\n",
    "        self.cache = {}\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Custom completion implementation.\"\"\"\n",
    "        self.request_count += 1\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = f\"{prompt}:{json.dumps(kwargs, sort_keys=True)}\"\n",
    "        if cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        # Mock API call\n",
    "        response = f\"Custom response to: {prompt[:50]}...\"\n",
    "        \n",
    "        # Cache response\n",
    "        self.cache[cache_key] = response\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def chat(self, messages: list[dict], **kwargs) -> str:\n",
    "        \"\"\"Custom chat implementation.\"\"\"\n",
    "        # Convert messages to prompt\n",
    "        prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "        return self.complete(prompt, **kwargs)\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get client statistics.\"\"\"\n",
    "        return {\n",
    "            \"requests\": self.request_count,\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"cache_hit_rate\": len(self.cache) / max(self.request_count, 1)\n",
    "        }\n",
    "\n",
    "# Local model client\n",
    "class LocalModelClient(LLMClient):\n",
    "    \"\"\"Client for local models (e.g., llama.cpp, GGML).\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model_path = model_path\n",
    "        # In production, load actual model here\n",
    "        self.model_loaded = True\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Run local model inference.\"\"\"\n",
    "        if not self.model_loaded:\n",
    "            raise RuntimeError(\"Model not loaded\")\n",
    "        \n",
    "        # Mock local inference\n",
    "        # In production, use actual model inference\n",
    "        return f\"Local model response: {prompt[:30]}...\"\n",
    "    \n",
    "    def chat(self, messages: list[dict], **kwargs) -> str:\n",
    "        \"\"\"Chat with local model.\"\"\"\n",
    "        # Format messages for local model\n",
    "        formatted_prompt = self._format_messages(messages)\n",
    "        return self.complete(formatted_prompt, **kwargs)\n",
    "    \n",
    "    def _format_messages(self, messages: list[dict]) -> str:\n",
    "        \"\"\"Format messages for local model.\"\"\"\n",
    "        formatted = []\n",
    "        for msg in messages:\n",
    "            if msg['role'] == 'system':\n",
    "                formatted.append(f\"System: {msg['content']}\")\n",
    "            elif msg['role'] == 'user':\n",
    "                formatted.append(f\"Human: {msg['content']}\")\n",
    "            elif msg['role'] == 'assistant':\n",
    "                formatted.append(f\"Assistant: {msg['content']}\")\n",
    "        \n",
    "        formatted.append(\"Assistant:\")  # Prompt for response\n",
    "        return \"\\n\\n\".join(formatted)\n",
    "\n",
    "# Test custom clients\n",
    "custom_client = CustomLLMClient(\n",
    "    endpoint=\"https://api.custom-llm.com\",\n",
    "    api_key=\"test-key\"\n",
    ")\n",
    "\n",
    "# Make requests\n",
    "response1 = custom_client.complete(\"Hello, world!\")\n",
    "response2 = custom_client.complete(\"Hello, world!\")  # Should hit cache\n",
    "response3 = custom_client.complete(\"Different prompt\")\n",
    "\n",
    "print(\"Custom Client Stats:\")\n",
    "print(json.dumps(custom_client.get_stats(), indent=2))\n",
    "\n",
    "# Test local model client\n",
    "local_client = LocalModelClient(model_path=\"/path/to/model.gguf\")\n",
    "local_response = local_client.chat([\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"}\n",
    "])\n",
    "print(f\"\\nLocal model response: {local_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Prompt Engineering\n",
    "\n",
    "Sophisticated prompt techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain of Thought prompting\n",
    "class ChainOfThoughtTemplate(PromptTemplate):\n",
    "    \"\"\"Template that enforces chain-of-thought reasoning.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_template: str, **defaults):\n",
    "        # Wrap template with CoT instructions\n",
    "        cot_template = f\"\"\"{base_template}\n",
    "\n",
    "Let's think step by step:\n",
    "1. First, I'll identify the key components of the problem\n",
    "2. Then, I'll analyze each component\n",
    "3. Finally, I'll synthesize a solution\n",
    "\n",
    "Step-by-step reasoning:\n",
    "\"\"\"\n",
    "        super().__init__(cot_template, **defaults)\n",
    "\n",
    "# Few-shot learning template\n",
    "class FewShotTemplate:\n",
    "    \"\"\"Template for few-shot learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, task_description: str):\n",
    "        self.task_description = task_description\n",
    "        self.examples = []\n",
    "    \n",
    "    def add_example(self, input_text: str, output_text: str):\n",
    "        \"\"\"Add an example.\"\"\"\n",
    "        self.examples.append({\"input\": input_text, \"output\": output_text})\n",
    "    \n",
    "    def format(self, input_text: str) -> str:\n",
    "        \"\"\"Format with examples.\"\"\"\n",
    "        prompt_parts = [self.task_description, \"\\nExamples:\\n\"]\n",
    "        \n",
    "        for i, example in enumerate(self.examples, 1):\n",
    "            prompt_parts.append(f\"Example {i}:\")\n",
    "            prompt_parts.append(f\"Input: {example['input']}\")\n",
    "            prompt_parts.append(f\"Output: {example['output']}\\n\")\n",
    "        \n",
    "        prompt_parts.append(\"Now, process this input:\")\n",
    "        prompt_parts.append(f\"Input: {input_text}\")\n",
    "        prompt_parts.append(\"Output:\")\n",
    "        \n",
    "        return \"\\n\".join(prompt_parts)\n",
    "\n",
    "# Self-consistency prompting\n",
    "class SelfConsistencyPrompt:\n",
    "    \"\"\"Generate multiple solutions and select the best.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: LLMClient, num_samples: int = 3):\n",
    "        self.client = client\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def generate(self, prompt: str) -> dict:\n",
    "        \"\"\"Generate multiple solutions.\"\"\"\n",
    "        solutions = []\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            # Add variation to prompt\n",
    "            varied_prompt = f\"{prompt}\\n\\n(Solution {i+1}:)\"\n",
    "            solution = self.client.complete(varied_prompt, temperature=0.7)\n",
    "            solutions.append(solution)\n",
    "        \n",
    "        # Analyze solutions\n",
    "        analysis_prompt = f\"\"\"Here are {len(solutions)} solutions to a problem:\n",
    "\n",
    "{chr(10).join([f'Solution {i+1}: {s}' for i, s in enumerate(solutions)])}\n",
    "\n",
    "Which solution is best and why? Provide a brief analysis.\"\"\"\n",
    "        \n",
    "        analysis = self.client.complete(analysis_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"solutions\": solutions,\n",
    "            \"analysis\": analysis,\n",
    "            \"best_solution\": solutions[0]  # In production, parse analysis\n",
    "        }\n",
    "\n",
    "# Test advanced prompting\n",
    "print(\"Chain of Thought Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cot_template = ChainOfThoughtTemplate(\n",
    "    \"Solve this problem: {problem}\"\n",
    ")\n",
    "\n",
    "cot_response = client.complete(\n",
    "    cot_template.format(problem=\"If a train travels 120 miles in 2 hours, how far will it travel in 5 hours at the same speed?\")\n",
    ")\n",
    "print(cot_response)\n",
    "\n",
    "print(\"\\n\\nFew-Shot Learning Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "few_shot = FewShotTemplate(\"Convert natural language to SQL queries:\")\n",
    "few_shot.add_example(\n",
    "    \"Show all users\",\n",
    "    \"SELECT * FROM users;\"\n",
    ")\n",
    "few_shot.add_example(\n",
    "    \"Find users named John\",\n",
    "    \"SELECT * FROM users WHERE name = 'John';\"\n",
    ")\n",
    "few_shot.add_example(\n",
    "    \"Count active users\",\n",
    "    \"SELECT COUNT(*) FROM users WHERE status = 'active';\"\n",
    ")\n",
    "\n",
    "sql_prompt = few_shot.format(\"Get emails of premium users\")\n",
    "sql_response = client.complete(sql_prompt)\n",
    "print(f\"Generated SQL: {sql_response}\")\n",
    "\n",
    "print(\"\\n\\nSelf-Consistency Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "self_consistency = SelfConsistencyPrompt(client, num_samples=3)\n",
    "result = self_consistency.generate(\n",
    "    \"What's the best way to learn programming?\"\n",
    ")\n",
    "\n",
    "print(\"Multiple solutions generated:\")\n",
    "for i, solution in enumerate(result['solutions'], 1):\n",
    "    print(f\"\\nSolution {i}: {solution[:100]}...\")\n",
    "\n",
    "print(f\"\\nAnalysis: {result['analysis'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Optimization\n",
    "\n",
    "Techniques for optimizing AILib applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response caching\n",
    "class CachedClient(LLMClient):\n",
    "    \"\"\"LLM client with intelligent caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_client: LLMClient, cache_size: int = 100):\n",
    "        self.base_client = base_client\n",
    "        self.cache = {}\n",
    "        self.cache_size = cache_size\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _get_cache_key(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Generate cache key.\"\"\"\n",
    "        # Include relevant kwargs in key\n",
    "        key_parts = [prompt]\n",
    "        for k in ['temperature', 'max_tokens', 'model']:\n",
    "            if k in kwargs:\n",
    "                key_parts.append(f\"{k}:{kwargs[k]}\")\n",
    "        return \"|\".join(key_parts)\n",
    "    \n",
    "    def complete(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Cached completion.\"\"\"\n",
    "        cache_key = self._get_cache_key(prompt, **kwargs)\n",
    "        \n",
    "        if cache_key in self.cache:\n",
    "            self.hits += 1\n",
    "            return self.cache[cache_key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        response = self.base_client.complete(prompt, **kwargs)\n",
    "        \n",
    "        # Manage cache size\n",
    "        if len(self.cache) >= self.cache_size:\n",
    "            # Remove oldest entry (simple FIFO)\n",
    "            oldest = next(iter(self.cache))\n",
    "            del self.cache[oldest]\n",
    "        \n",
    "        self.cache[cache_key] = response\n",
    "        return response\n",
    "    \n",
    "    def chat(self, messages: list[dict], **kwargs) -> str:\n",
    "        \"\"\"Cached chat.\"\"\"\n",
    "        # Convert messages to string for caching\n",
    "        prompt = json.dumps(messages)\n",
    "        return self.complete(prompt, **kwargs)\n",
    "    \n",
    "    def get_cache_stats(self) -> dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        return {\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"hit_rate\": self.hits / total if total > 0 else 0,\n",
    "            \"cache_size\": len(self.cache)\n",
    "        }\n",
    "\n",
    "# Batch processing optimization\n",
    "class BatchProcessor:\n",
    "    \"\"\"Process multiple requests efficiently.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: LLMClient, batch_size: int = 5):\n",
    "        self.client = client\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def process_batch(self, items: list, prompt_template: PromptTemplate) -> list:\n",
    "        \"\"\"Process items in batches.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(items), self.batch_size):\n",
    "            batch = items[i:i + self.batch_size]\n",
    "            \n",
    "            # Process batch in parallel using threads\n",
    "            with ThreadPoolExecutor(max_workers=self.batch_size) as executor:\n",
    "                futures = []\n",
    "                for item in batch:\n",
    "                    prompt = prompt_template.format(item=item)\n",
    "                    future = executor.submit(self.client.complete, prompt)\n",
    "                    futures.append(future)\n",
    "                \n",
    "                # Collect results\n",
    "                for future in futures:\n",
    "                    results.append(future.result())\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Token optimization\n",
    "class TokenOptimizer:\n",
    "    \"\"\"Optimize token usage.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compress_prompt(prompt: str, max_length: int = 1000) -> str:\n",
    "        \"\"\"Compress prompt to save tokens.\"\"\"\n",
    "        if len(prompt) <= max_length:\n",
    "            return prompt\n",
    "        \n",
    "        # Smart truncation - keep beginning and end\n",
    "        start_length = max_length // 2\n",
    "        end_length = max_length // 2\n",
    "        \n",
    "        return f\"{prompt[:start_length]}... [truncated] ...{prompt[-end_length:]}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_tokens(text: str) -> int:\n",
    "        \"\"\"Estimate token count (rough approximation).\"\"\"\n",
    "        # Rough estimate: ~1 token per 4 characters\n",
    "        return len(text) // 4\n",
    "\n",
    "# Test performance optimizations\n",
    "print(\"Testing Cached Client:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cached_client = CachedClient(client)\n",
    "\n",
    "# Make repeated requests\n",
    "test_prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"What is JavaScript?\",\n",
    "    \"What is Python?\",  # Should hit cache\n",
    "    \"What is Java?\",\n",
    "    \"What is Python?\"   # Should hit cache\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    start = time.time()\n",
    "    response = cached_client.complete(prompt)\n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Prompt: {prompt} - Time: {elapsed:.3f}s\")\n",
    "\n",
    "print(\"\\nCache Statistics:\")\n",
    "print(json.dumps(cached_client.get_cache_stats(), indent=2))\n",
    "\n",
    "# Test batch processing\n",
    "print(\"\\n\\nTesting Batch Processing:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "batch_processor = BatchProcessor(client, batch_size=3)\n",
    "items = [f\"Item {i}\" for i in range(10)]\n",
    "template = PromptTemplate(\"Describe {item} in one word\")\n",
    "\n",
    "start = time.time()\n",
    "results = batch_processor.process_batch(items, template)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Processed {len(items)} items in {elapsed:.2f}s\")\n",
    "print(f\"Average time per item: {elapsed/len(items):.3f}s\")\n",
    "\n",
    "# Test token optimization\n",
    "print(\"\\n\\nTesting Token Optimization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "long_prompt = \"This is a very long prompt \" * 100\n",
    "compressed = TokenOptimizer.compress_prompt(long_prompt, max_length=100)\n",
    "\n",
    "print(f\"Original length: {len(long_prompt)} chars\")\n",
    "print(f\"Original tokens (est): {TokenOptimizer.estimate_tokens(long_prompt)}\")\n",
    "print(f\"Compressed length: {len(compressed)} chars\")\n",
    "print(f\"Compressed tokens (est): {TokenOptimizer.estimate_tokens(compressed)}\")\n",
    "print(f\"Token savings: {(1 - len(compressed)/len(long_prompt))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Patterns\n",
    "\n",
    "Integrate AILib with other systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database integration\n",
    "class DatabaseAgent:\n",
    "    \"\"\"Agent that interacts with databases.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: LLMClient):\n",
    "        self.client = client\n",
    "        self.tools = self._create_db_tools()\n",
    "        self.agent = Agent(llm=client, tools=self.tools)\n",
    "    \n",
    "    def _create_db_tools(self) -> ToolRegistry:\n",
    "        \"\"\"Create database tools.\"\"\"\n",
    "        registry = ToolRegistry()\n",
    "        \n",
    "        @tool(registry=registry)\n",
    "        def query_database(sql: str) -> list:\n",
    "            \"\"\"Execute SQL query (mock).\"\"\"\n",
    "            # In production, use actual database connection\n",
    "            mock_results = [\n",
    "                {\"id\": 1, \"name\": \"Alice\", \"role\": \"Engineer\"},\n",
    "                {\"id\": 2, \"name\": \"Bob\", \"role\": \"Manager\"}\n",
    "            ]\n",
    "            return mock_results\n",
    "        \n",
    "        @tool(registry=registry)\n",
    "        def describe_table(table_name: str) -> dict:\n",
    "            \"\"\"Get table schema.\"\"\"\n",
    "            schemas = {\n",
    "                \"users\": {\n",
    "                    \"columns\": [\"id\", \"name\", \"email\", \"role\", \"created_at\"],\n",
    "                    \"primary_key\": \"id\"\n",
    "                },\n",
    "                \"orders\": {\n",
    "                    \"columns\": [\"id\", \"user_id\", \"total\", \"status\", \"created_at\"],\n",
    "                    \"primary_key\": \"id\"\n",
    "                }\n",
    "            }\n",
    "            return schemas.get(table_name, {\"error\": \"Table not found\"})\n",
    "        \n",
    "        return registry\n",
    "    \n",
    "    def analyze_data(self, request: str) -> str:\n",
    "        \"\"\"Analyze data based on natural language request.\"\"\"\n",
    "        return self.agent.run(request)\n",
    "\n",
    "# API integration\n",
    "class APIIntegration:\n",
    "    \"\"\"Integrate with external APIs.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: LLMClient):\n",
    "        self.client = client\n",
    "        self.api_specs = {}\n",
    "    \n",
    "    def register_api(self, name: str, spec: dict):\n",
    "        \"\"\"Register an API specification.\"\"\"\n",
    "        self.api_specs[name] = spec\n",
    "    \n",
    "    def generate_api_call(self, request: str) -> dict:\n",
    "        \"\"\"Generate API call from natural language.\"\"\"\n",
    "        # Build prompt with API specs\n",
    "        prompt = f\"\"\"\n",
    "Available APIs:\n",
    "{json.dumps(self.api_specs, indent=2)}\n",
    "\n",
    "User request: {request}\n",
    "\n",
    "Generate the appropriate API call as JSON:\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.client.complete(prompt)\n",
    "        \n",
    "        # Parse response (in production, add error handling)\n",
    "        try:\n",
    "            api_call = json.loads(response)\n",
    "            return api_call\n",
    "        except:\n",
    "            return {\"error\": \"Failed to generate API call\"}\n",
    "\n",
    "# Event-driven integration\n",
    "class EventDrivenAgent:\n",
    "    \"\"\"Agent that responds to events.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: LLMClient):\n",
    "        self.client = client\n",
    "        self.event_handlers = {}\n",
    "        self.event_history = []\n",
    "    \n",
    "    def register_handler(self, event_type: str, handler_prompt: str):\n",
    "        \"\"\"Register an event handler.\"\"\"\n",
    "        self.event_handlers[event_type] = handler_prompt\n",
    "    \n",
    "    def handle_event(self, event: dict) -> str:\n",
    "        \"\"\"Handle an incoming event.\"\"\"\n",
    "        event_type = event.get('type')\n",
    "        \n",
    "        if event_type not in self.event_handlers:\n",
    "            return \"No handler for event type\"\n",
    "        \n",
    "        # Get handler prompt\n",
    "        handler_prompt = self.event_handlers[event_type]\n",
    "        \n",
    "        # Build context\n",
    "        context = f\"\"\"\n",
    "Event: {json.dumps(event, indent=2)}\n",
    "Recent events: {json.dumps(self.event_history[-5:], indent=2)}\n",
    "\n",
    "{handler_prompt}\n",
    "\"\"\"\n",
    "        \n",
    "        response = self.client.complete(context)\n",
    "        \n",
    "        # Store event\n",
    "        self.event_history.append({\n",
    "            \"event\": event,\n",
    "            \"response\": response,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Test integrations\n",
    "print(\"Database Agent Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "db_agent = DatabaseAgent(client)\n",
    "result = db_agent.analyze_data(\n",
    "    \"Show me all users who are engineers\"\n",
    ")\n",
    "print(f\"Query result: {result}\")\n",
    "\n",
    "print(\"\\n\\nAPI Integration Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "api_integration = APIIntegration(client)\n",
    "api_integration.register_api(\"weather\", {\n",
    "    \"endpoint\": \"/weather\",\n",
    "    \"params\": {\"city\": \"string\", \"units\": \"string\"}\n",
    "})\n",
    "api_integration.register_api(\"stocks\", {\n",
    "    \"endpoint\": \"/stocks\",\n",
    "    \"params\": {\"symbol\": \"string\", \"period\": \"string\"}\n",
    "})\n",
    "\n",
    "api_call = api_integration.generate_api_call(\n",
    "    \"Get the weather in New York in celsius\"\n",
    ")\n",
    "print(f\"Generated API call: {json.dumps(api_call, indent=2)}\")\n",
    "\n",
    "print(\"\\n\\nEvent-Driven Agent Example:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "event_agent = EventDrivenAgent(client)\n",
    "event_agent.register_handler(\n",
    "    \"user_signup\",\n",
    "    \"Generate a welcome message for the new user\"\n",
    ")\n",
    "event_agent.register_handler(\n",
    "    \"error\",\n",
    "    \"Analyze the error and suggest a solution\"\n",
    ")\n",
    "\n",
    "# Handle events\n",
    "signup_event = {\n",
    "    \"type\": \"user_signup\",\n",
    "    \"user\": {\"name\": \"Alice\", \"email\": \"alice@example.com\"}\n",
    "}\n",
    "response = event_agent.handle_event(signup_event)\n",
    "print(f\"Welcome message: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Error Handling\n",
    "\n",
    "Robust error handling strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Retry with exponential backoff\nclass RetryClient(LLMClient):\n    \"\"\"Client with automatic retry logic.\"\"\"\n    \n    def __init__(self, base_client: LLMClient, max_retries: int = 3):\n        self.base_client = base_client\n        self.max_retries = max_retries\n    \n    def complete(self, prompt: str, **kwargs) -> str:\n        \"\"\"Complete with retry.\"\"\"\n        last_error = None\n        \n        for attempt in range(self.max_retries):\n            try:\n                return self.base_client.complete(prompt, **kwargs)\n            except Exception as e:\n                last_error = e\n                \n                # Exponential backoff\n                wait_time = 2 ** attempt\n                print(f\"Retry {attempt + 1}/{self.max_retries} after {wait_time}s...\")\n                time.sleep(wait_time)\n        \n        raise Exception(f\"Failed after {self.max_retries} retries: {last_error}\")\n    \n    def chat(self, messages: list[dict], **kwargs) -> str:\n        \"\"\"Chat with retry.\"\"\"\n        # Similar retry logic\n        return self.complete(json.dumps(messages), **kwargs)\n\n# Circuit breaker pattern\nclass CircuitBreakerClient(LLMClient):\n    \"\"\"Client with circuit breaker for fault tolerance.\"\"\"\n    \n    def __init__(self, base_client: LLMClient, failure_threshold: int = 5):\n        self.base_client = base_client\n        self.failure_threshold = failure_threshold\n        self.failure_count = 0\n        self.is_open = False\n        self.last_failure_time = None\n        self.reset_timeout = 60  # seconds\n    \n    def _check_circuit(self):\n        \"\"\"Check if circuit should be reset.\"\"\"\n        if self.is_open and self.last_failure_time:\n            if time.time() - self.last_failure_time > self.reset_timeout:\n                print(\"Circuit breaker reset\")\n                self.is_open = False\n                self.failure_count = 0\n    \n    def complete(self, prompt: str, **kwargs) -> str:\n        \"\"\"Complete with circuit breaker.\"\"\"\n        self._check_circuit()\n        \n        if self.is_open:\n            raise Exception(\"Circuit breaker is open - service unavailable\")\n        \n        try:\n            response = self.base_client.complete(prompt, **kwargs)\n            # Success - reset failure count\n            self.failure_count = 0\n            return response\n        except Exception as e:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n            \n            if self.failure_count >= self.failure_threshold:\n                print(f\"Circuit breaker opened after {self.failure_count} failures\")\n                self.is_open = True\n            \n            raise e\n    \n    def chat(self, messages: list[dict], **kwargs) -> str:\n        \"\"\"Chat with circuit breaker.\"\"\"\n        return self.complete(json.dumps(messages), **kwargs)\n\n# Fallback strategies\nclass FallbackClient(LLMClient):\n    \"\"\"Client with fallback options.\"\"\"\n    \n    def __init__(self, primary_client: LLMClient, fallback_client: LLMClient):\n        self.primary_client = primary_client\n        self.fallback_client = fallback_client\n    \n    def complete(self, prompt: str, **kwargs) -> str:\n        \"\"\"Complete with fallback.\"\"\"\n        try:\n            return self.primary_client.complete(prompt, **kwargs)\n        except Exception as e:\n            print(f\"Primary client failed: {e}\")\n            print(\"Falling back to secondary client...\")\n            return self.fallback_client.complete(prompt, **kwargs)\n    \n    def chat(self, messages: list[dict], **kwargs) -> str:\n        \"\"\"Chat with fallback.\"\"\"\n        try:\n            return self.primary_client.chat(messages, **kwargs)\n        except Exception:\n            return self.fallback_client.chat(messages, **kwargs)\n\nprint(\"Advanced Error Handling Examples:\")\nprint(\"=\" * 50)\n\n# Demonstrate retry client\nprint(\"\\nRetry Client (simulated):\")\nprint(\"Would retry failed requests with exponential backoff\")\n\n# Demonstrate circuit breaker\nprint(\"\\nCircuit Breaker (simulated):\")\nprint(\"Would open circuit after 5 failures, preventing cascading failures\")\n\n# Demonstrate fallback\nprint(\"\\nFallback Client (simulated):\")\nprint(\"Would automatically switch to backup LLM if primary fails\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Pydantic Validation\n\nAILib provides comprehensive validation using Pydantic for type safety and data integrity:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Summary\n\nIn this tutorial, you learned advanced AILib features:\n\n- âœ… Async operations for better performance\n- âœ… Streaming responses for real-time output\n- âœ… Creating custom LLM clients\n- âœ… Advanced prompt engineering techniques\n- âœ… Performance optimization strategies\n- âœ… Integration patterns with external systems\n- âœ… Robust error handling approaches\n- âœ… Pydantic validation for type safety and data integrity\n\nThese advanced features enable you to:\n- Build scalable AI applications\n- Integrate with existing systems\n- Handle edge cases gracefully\n- Optimize for performance and cost\n- Ensure data quality with validation\n\n## Next Steps\n\nReady for the final tutorial?\n\n- **Tutorial 10: Real-World Examples** - Complete applications using everything you've learned\n\nHappy building! ðŸš€",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned advanced AILib features:\n",
    "\n",
    "- âœ… Async operations for better performance\n",
    "- âœ… Streaming responses for real-time output\n",
    "- âœ… Creating custom LLM clients\n",
    "- âœ… Advanced prompt engineering techniques\n",
    "- âœ… Performance optimization strategies\n",
    "- âœ… Integration patterns with external systems\n",
    "- âœ… Robust error handling approaches\n",
    "\n",
    "These advanced features enable you to:\n",
    "- Build scalable AI applications\n",
    "- Integrate with existing systems\n",
    "- Handle edge cases gracefully\n",
    "- Optimize for performance and cost\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Ready for the final tutorial?\n",
    "\n",
    "- **Tutorial 10: Real-World Examples** - Complete applications using everything you've learned\n",
    "\n",
    "Happy building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
