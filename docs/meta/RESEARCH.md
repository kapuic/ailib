# Developing LLM Applications: Key Frameworks and Best Practices

## Platforms and Frameworks for LLM Applications

Building applications powered by large language models (LLMs) is greatly facilitated by specialized frameworks and platforms. These tools provide abstractions for chaining model calls, managing prompts, integrating external data, and orchestrating complex workflows. Below we survey some of the most prominent frameworks, including both global open-source projects and notable domestic platforms, along with their characteristics:

### Leading Open-Source Frameworks (Global)

- **LangChain** (Python & JavaScript; GitHub ⭐ ~70k) – *LangChain* is one of the earliest and most popular libraries for LLM application development[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=2). It introduces abstractions like *chains* (sequences of prompts/LLM calls) and *agents* (LLMs that use tools) to create context-aware reasoning applications. LangChain supports a wide range of integrations (multiple LLM providers, vector databases, APIs) and has a rich ecosystem (e.g. LangSmith for monitoring)[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=,aware%2C reasoning applications �). Its versatility and integration of retrieval-augmented generation (RAG), memory, and tools helped it gain widespread adoption in diverse LLM use cases[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=,aware%2C reasoning applications �).
- **LlamaIndex** (Python; GitHub ⭐ ~30k) – *LlamaIndex* (formerly GPT Index) focuses on **retrieval-augmented generation (RAG)** workflows[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=Why It's Popular%3A). It provides data connectors and index structures to ingest documents and enable LLMs to query them efficiently. Developers can build pipelines where user queries are mapped to relevant chunks of data (via indices) which are then fed into LLM prompts. LlamaIndex seamlessly integrates with LangChain and various vector stores, specializing in knowledge-intensive apps[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=* Specializes in retrieval,RAG).
- **Haystack** (Python; GitHub ⭐ ~25k) – *Haystack* by deepset is a mature open-source framework originally built for neural search and QA, now extended for generative AI pipelines[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=Why It's Trending%3A). It follows a pipeline-centric approach: you can define nodes for querying, filtering, reader (LLM) etc., and chain them. Haystack excels in building production-ready QA systems over documents, with robust evaluation tools and support for hybrid search (keyword + vector)[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=Why It's Trending%3A). It also supports prompt templates and generative components, making it useful for both retrieval-based and open-ended QA tasks.
- **Guidance** (Python; Microsoft/OpenAI) – *Guidance* is a framework that allows fine-grained control of LLM generation via a templating language. It lets you intermix prose, model calls, and Python logic in a single prompt template, guiding the LLM to produce structured outputs or follow certain constraints. Guidance can enforce JSON formats or tags by verifying partial outputs as they stream. This gives developers more deterministic control over LLM reasoning steps and output formatting, which is otherwise handled implicitly in simpler frameworks. (For example, you can ensure an LLM’s answer contains certain fields by templating them and validating in-loop.)
- **AutoGen** (Python; GitHub ⭐ ~55k) – *AutoGen* is an open-source framework from Microsoft for creating **multi-agent conversations** and workflows[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=3). It enables defining multiple LLM “agents” (with roles like assistant, user-proxy, tools, etc.) that converse with each other to solve tasks. AutoGen provides higher-level patterns for agents to cooperate or specialize (e.g. one agent can be a reasoning planner, another an executor). It also supports tool integration and features an **AutoGen Studio** GUI for visualizing and prototyping agent dialogues[github.com](https://github.com/microsoft/autogen#:~:text=,U "autogenstudio)[github.com](https://github.com/microsoft/autogen#:~:text=Web Browsing Agent Team). AutoGen is known for handling asynchronous interactions well (agents can work concurrently), and for achieving reliable tool use by the agents. Its focus on multi-agent orchestration helped it quickly gain popularity[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=Why It's Trending%3A) as developers explore agent-based AI workflows.
- **CrewAI** (Python; GitHub ⭐ ~45k) – *CrewAI* is a newer framework geared towards **collaborative multi-agent systems**[github.com](https://github.com/crewAIInc/crewAI#:~:text=Fast and Flexible Multi,Framework). It was built from scratch (not on LangChain) to enable multiple specialized agents (“crew members”) working together on complex tasks. CrewAI emphasizes high performance and fine control: it allows low-level customization of agent behaviors and uses an event-driven architecture (its *Flows* and *Crews* abstractions) for flexible orchestration[github.com](https://github.com/crewAIInc/crewAI#:~:text=,agents tailored to any scenario)[github.com](https://github.com/crewAIInc/crewAI#:~:text=CrewAI unlocks the true potential,Agents or Flows of Events). The project highlights enterprise-ready features (observability, an optional control plane for monitoring deployments[github.com](https://github.com/crewAIInc/crewAI#:~:text=You can try one part,Crew Control Plane for free)) and claims a rapidly growing community of developers. CrewAI’s rise reflects the trend of multi-agent autonomy, especially for complex enterprise workflows requiring both **autonomy and coordination**[github.com](https://github.com/crewAIInc/crewAI#:~:text=,grade scenarios).
- **Flowise** (TypeScript/Node; GitHub ⭐ ~42k) – *Flowise* is an open-source **low-code** tool that lets you build LLM apps through a visual **drag-and-drop interface**[github.com](https://github.com/FlowiseAI/Flowise#:~:text=,Star 41.8k)[github.com](https://github.com/FlowiseAI/Flowise#:~:text=41,59  Activity). It provides a node-based canvas where each node could be an LLM prompt, a tool (e.g. web search, database query), a conditional logic, etc., and you connect them to design a workflow. Flowise runs as a web app and executes these flows using an underlying engine (with LangChain JS under the hood for some nodes). It supports many LLM providers and allows chaining with minimal coding, making it popular for quick prototypes. However, Flowise currently focuses on linear flows and does **not support agentic tool use (no dynamic ReAct agent)** by default[github.com](https://github.com/langgenius/dify#:~:text=5,DALL·E%2C Stable Diffusion and WolframAlpha)[github.com](https://github.com/langgenius/dify#:~:text=Feature Dify,Deployment ✅ ✅ ✅ ❌). It’s well-suited for straightforward chatbots, data query workflows, and other deterministic pipelines, but for agent-style tool selection you might need to integrate a different approach.
- **LangFlow** (Python + React; GitHub ⭐ ~80k) – *LangFlow* offers a polished UI to construct and deploy LLM workflows visually, specifically built around LangChain components. Like Flowise, it presents a graph editor where you can drop nodes for prompts, chains, memory, tools, etc., then connect them to define the app’s logic. LangFlow supports all major LLMs and vector DBs, and it can **export flows as JSON or as Python code** for use in applications[github.com](https://github.com/langflow-ai/langflow#:~:text=,ready security and scalability)[github.com](https://github.com/langflow-ai/langflow#:~:text=,ready security and scalability). Notably, it includes features for multi-agent orchestration and conversation management out-of-the-box[github.com](https://github.com/langflow-ai/langflow#:~:text=,ready security and scalability). One powerful aspect of LangFlow is that every flow you create can be turned into an **endpoint or microservice** – it includes an API server to host your workflows, so external applications can call them. Thanks to an active community and its ease of use, LangFlow’s repository skyrocketed to tens of thousands of stars[x.com](https://x.com/ossalternative/status/1939806277556650430#:~:text=OpenAlternative on X%3A ,co%2FpUqCswm8jD), reflecting its status as a go-to no-code builder for LLM applications.
- **SuperAGI** (Python; GitHub ⭐ ~6k) – *SuperAGI* is an open-source framework focused on autonomous AI agents (in the spirit of "AutoGPT"). It provides a platform to create agents that can perform multi-step tasks, use tools, and attempt to achieve goals set by the user. SuperAGI includes a web UI to manage agents and an execution engine that handles planning and tool integrations (web browsing, file I/O, etc.). While it hasn’t reached LangChain-level popularity, it attracted interest by promising a more **“developer-first” approach to autonomous agents**, with an emphasis on reliability and extensibility. As of mid-2023 it had around 6k stars and some hype in the community[reddit.com](https://www.reddit.com/r/AutoGPT/comments/1451vm6/superagi_anyone_not_affiliated_with_it_tried_it/#:~:text=Anyone ,tried it out). SuperAGI can be a choice if your use-case leans towards long-running agents with memory and you want an off-the-shelf starting point instead of assembling pieces via LangChain.
- **Marvin** (Python; GitHub ⭐ ~3k) – *Marvin* is a newer library (from the PrefectHQ team) for building AI-powered software with an emphasis on **structured outputs and reliability**. Marvin lets you define “AI Functions” in Python, including a schema for their return types (using Pydantic data models). At runtime, it handles calling the LLM with appropriate prompts and parsing/validating the LLM’s output into the specified Pydantic schema, automatically retrying or correcting as needed. This gives developers a way to treat LLM calls more like regular function calls with typed returns – making the AI’s behavior more deterministic and testable. Marvin also supports tools and agent-like workflows, but its standout feature is the integration of output validation in a developer-friendly way. It’s an illustrative example of a framework focusing on *trustworthy outputs* rather than just chaining calls.

*(The above list is not exhaustive – other notable mentions include Hugging Face’s **Transformers** pipelines (which offer basic building blocks for text generation and Q&A), **OpenAI Evals** and **LangSmith** for evaluation/monitoring, Microsoft’s experimental **TypeChat** (TypeScript library for structured output with Zod), and many more. The ecosystem is rapidly evolving, with new frameworks frequently emerging.)*

### Domestic Platforms and Frameworks (China)

The Chinese AI community has developed its own rich set of LLM application platforms – both open-source projects and enterprise solutions – which cater to domestic needs (e.g. support for Chinese language models, self-hosting, and data privacy requirements). Here we highlight a few prominent examples:

- **Dify** (开源; Python/TypeScript; GitHub ⭐ 100k+) – *Dify* is a production-ready open-source platform for LLM application development[github.com](https://github.com/langgenius/dify#:~:text=Dify is an open,move from prototype to production). It provides an **intuitive web interface** and a comprehensive backend combining LLMOps and “Backend-as-a-Service” capabilities. With Dify, developers (or non-developers) can visually build AI applications by creating **workflows on a canvas** that include model prompts, tool calls, and branches[github.com](https://github.com/langgenius/dify#:~:text=Key features)[github.com](https://github.com/langgenius/dify#:~:text=5,DALL·E%2C Stable Diffusion and WolframAlpha). It supports hundreds of LLMs (proprietary and open) via a plugin-like provider system – including GPT-4, Mistral, Llama 3, Baichuan, etc. – and allows hybrid usage of local and API-hosted models[github.com](https://github.com/langgenius/dify#:~:text=all the following features and,beyond). Crucially, Dify has inbuilt support for Retrieval-Augmented Generation: you can ingest documents and set up a **RAG pipeline** with vector search in just a few clicks[github.com](https://github.com/langgenius/dify#:~:text=based app). It also enables defining **agents** either through OpenAI function calling or ReAct prompts, and comes with 50+ ready-made tools (web search, calculator, image generators, etc.) that agents can use[github.com](https://github.com/langgenius/dify#:~:text=5,DALL·E%2C Stable Diffusion and WolframAlpha). For production deployment, Dify offers features like monitoring/logging of prompts, user management, and team collaboration. Thanks to strong community adoption, Dify’s GitHub repo skyrocketed past **100,000 stars** by mid-2025[dify.ai](https://dify.ai/blog/100k-stars-on-github-thank-you-to-our-amazing-open-source-community#:~:text=Today%2C we're thrilled to announce,have contributed to Dify's growth), making it one of the top open-source AI projects globally. The popularity of Dify highlights the demand for an integrated, user-friendly LLM development platform that can be self-hosted – especially for enterprises concerned about data governance. *(Note: Dify is developed by the startup LangGenius and has both an open-source version and a managed cloud service. It exemplifies how “low-code” AI platforms can accelerate development while still offering extensibility via APIs.)*
- **Eino** (开源; Go; GitHub ⭐ ~5k) – *Eino* is a large-language-model application framework open-sourced by ByteDance in 2024[cloudwego.io](https://www.cloudwego.io/docs/eino/overview/bytedance_eino_practice/#:~:text=Eino is an open,comprehensive tool ecosystem%2C and). Written in Golang, it aims to be a *“full-code”* developer framework (as opposed to a no-code platform) that emphasizes type safety, composability, and performance for production services. Eino drew inspiration from frameworks like LangChain and LlamaIndex, but designed APIs that fit Go conventions and performance needs[github.com](https://github.com/cloudwego/eino#:~:text=Eino['aino] (pronounced similarly to ,aligns with Golang programming conventions)[github.com](https://github.com/cloudwego/eino#:~:text=can be easily reused and,to online tracing and evaluation). It provides a set of **reusable components** (LLM interface, prompt templates, vector stores, tools, etc.) and a powerful **composition engine** to orchestrate them. Notably, Eino supports building both simple linear chains and more complex directed graphs of operations (including cycles and branching)[github.com](https://github.com/cloudwego/eino#:~:text=API Characteristics and usage Chain,Powerful and flexible)[github.com](https://github.com/cloudwego/eino#:~:text=Now let's create a graph,execute those tools if needed). This allows developers to create non-trivial agent loops and tool-using workflows in a structured way, with the framework handling concurrency and data flow. It also includes strong **type-checking** at compile-time for these chains/graphs, preventing many errors. Although Eino is relatively new and its star count (~5k) reflects a niche community[github.com](https://github.com/cloudwego/eino#:~:text=5,Tags   Activity), it’s significant as a project proving out LLM orchestration in a systems programming context (Go). It also indicates interest from major tech companies (ByteDance) in open-sourcing their internal LLMOps tooling. For Go developers or high-performance server scenarios, Eino provides an intriguing alternative to Python-based frameworks.
- **Promptulate** (开源; Python; smaller community) – *Promptulate* is a lightweight Python framework created by developers in the Baidu Qianfan community, aiming to simplify building LLM *Agents* and automation. It allows you to construct agents in a “pythonic” way, specifying tools and behaviors with minimal boilerplate. Promptulate’s design is somewhat akin to a mini LangChain focused on agent use-cases. While its GitHub presence is modest (a few hundred stars), it’s notable as part of the domestic open-source innovation around LLMs. It supports Baidu’s ERNIE Bot and other local models, making it easier for Chinese developers to integrate those into agent loops.
- **Enterprise Cloud Platforms** – In addition to open-source projects, China’s tech giants have released comprehensive LLM development platforms on the cloud:
  - *Alibaba Cloud’s BaiChuan or “Bai Lian” (百炼) AI Platform* – an enterprise-grade suite on Alibaba Cloud for building and deploying big-model applications[aliyun.com](https://www.aliyun.com/product/bailian#:~:text=企业级大模型开发平台_百炼AI应用构建). It provides end-to-end tools from model training and fine-tuning to application hosting. Bai Lian integrates Alibaba’s own models (like Tongyi Qianwen) and offers a no-code interface to chain model calls, with a focus on corporate use cases (customer service, content generation, etc.). It emphasizes data security and easy integration with other Alibaba Cloud services.
  - *Baidu Qianfan (千帆) Platform* – Baidu’s one-stop large-model development and service platform[python.langchain.com](https://python.langchain.com/docs/integrations/llms/baidu_qianfan_endpoint/#:~:text=Baidu Qianfan ,operation platform for enterprise developers), which supports their ERNIE models and others. Qianfan allows developers to do prompt engineering, fine-tune models on custom data, and deploy via APIs. It also launched a no-code prompt/interface builder (somewhat analogous to OpenAI’s ChatGPT Plugins interface or MS Prompt Flow) to speed up creating chatbots. Baidu’s ecosystem approach means Qianfan ties in with their search, cloud, and productivity apps.
  - *Huawei* – Huawei’s **MindSpore** framework (an AI training framework) and the related **PanGu** large model are complemented by an application development platform (Ascend AI cloud services) introduced in 2023[zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/681459152#:~:text=2024！国内AI大模型平台哪家强？全方面测评来了 ,). Huawei’s platform targets industries (finance, manufacturing, etc.) needing on-premise solutions, offering a full pipeline from model training to deployment, with an emphasis on leveraging Huawei hardware (Ascend AI chips). While less publicly accessible, it addresses the demand for *self-contained domestic AI solutions* due to data sovereignty concerns.

These domestic platforms often mirror the functionality of Western counterparts like Azure OpenAI Studio or AWS Bedrock, but with local model support and compliance with Chinese regulations. They may not all be open source, but they are important for enterprise adoption. Notably, the open-source Dify project has positioned itself as a **domestic alternative to OpenAI’s forthcoming GPTs/Assistants platform or LangChain**, boasting features like visual workflow design, integrated vector DB, and one-click deployment that align well with Chinese enterprise needs[github.com](https://github.com/langgenius/dify#:~:text=Feature Dify,Deployment ✅ ✅ ✅ ❌). The rapid rise of Dify (100k+ stars)[dify.ai](https://dify.ai/blog/100k-stars-on-github-thank-you-to-our-amazing-open-source-community#:~:text=Today%2C we're thrilled to announce,have contributed to Dify's growth) and LangFlow (tens of thousands of stars)[x.com](https://x.com/ossalternative/status/1939806277556650430#:~:text=OpenAlternative on X%3A ,co%2FpUqCswm8jD) in the community shows that developers value accessible tools that can bridge the gap between complex LLM capabilities and real-world application requirements.

#### Comparison of Major LLM Development Tools

The following table provides a high-level comparison of the frameworks and platforms discussed, summarizing their core attributes:

| **Name**              | **Type & License**                   | **Primary Language**            | **GitHub Stars**                                             | **Key Features & Capabilities**                              |
| --------------------- | ------------------------------------ | ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **LangChain**         | Library (open-source, MIT)           | Python; JS/TS                   | ~70k[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=2) | Chains of prompts & tools; extensive integrations (LLMs, vector DBs, APIs); supports RAG, memory, agents (ReAct); large community/ecosystem. |
| **LlamaIndex**        | Library (open-source)                | Python                          | ~30k[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=GitHub Stars%3A 30%2C000%2B) | Data framework for RAG; indexes for documents; efficient retrieval and chunking; easy integration with LangChain and various databases. |
| **Haystack**          | Framework (open-source, Apache)      | Python                          | ~25k[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=5) | Modular pipelines for QA/search; supports hybrid search and generative QA; evaluation tools; production-ready (used in many NLP apps). |
| **Guidance**          | Library (open-source)                | Python                          | ~4k (est.)                                                   | Templating for fine-grained control of LLM output; can enforce formats with partial validation; good for structured prompt-programming. |
| **AutoGen**           | Framework (open-source, MIT)         | Python                          | ~55k[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=3) | Multi-agent conversations; agent teams with role specialization; tool integration; supports async and concurrent agents; comes with GUI (AutoGen Studio). |
| **CrewAI**            | Framework (open-source)              | Python                          | ~45k[linkedin.com](https://www.linkedin.com/pulse/ai-agent-frameworks-trends-popularity-2025-sachin-tiwari-c5tuc#:~:text=GitHub Stars%3A 45%2C000%2B) | High-performance multi-agent framework; custom “crew” and “flow” abstractions; independent of LangChain; enterprise features (observability, control plane); emphasizes collaborative agents. |
| **Flowise**           | Low-code platform (open-source)      | TypeScript/Node                 | ~42k[github.com](https://github.com/FlowiseAI/Flowise#:~:text=,Star 41.8k) | Visual drag-drop builder; supports many LLMs via APIs; best for linear workflows and chatbots; **no** native agent/tool loop (no dynamic decision-making)[github.com](https://github.com/langgenius/dify#:~:text=Feature Dify,Deployment ✅ ✅ ✅ ❌); easy to deploy via Docker. |
| **LangFlow**          | Low-code platform (open-source, MIT) | Python (FastAPI) + TS (React)   | ~80k[x.com](https://x.com/ossalternative/status/1939806277556650430#:~:text=OpenAlternative on X%3A ,co%2FpUqCswm8jD) | Visual interface for LangChain flows; supports multi-agent and RAG setups; can deploy flows as services; strong community adoption; offers enterprise-friendly features (auth, logging via integrations). |
| **SuperAGI**          | Framework (open-source)              | Python                          | ~6k[reddit.com](https://www.reddit.com/r/AutoGPT/comments/1451vm6/superagi_anyone_not_affiliated_with_it_tried_it/#:~:text=Anyone ,tried it out) | Autonomous AI agent platform; comes with tool integrations and memory; web UI to manage agents; aims for long-running task automation (inspired by AutoGPT). |
| **Marvin**            | Library (open-source, MIT)           | Python                          | ~3k (2025)                                                   | AI functions with output schemas (Pydantic validation); ensures structured outputs from LLMs; integrates with external tools; focuses on reliable and testable AI behaviors. |
| **Dify**              | Platform (open-source, Apache-2.0)   | Python (FastAPI) + TS (Next.js) | 100k+[dify.ai](https://dify.ai/blog/100k-stars-on-github-thank-you-to-our-amazing-open-source-community#:~:text=Today%2C we're thrilled to announce,have contributed to Dify's growth) | Full-stack LLMOps platform; **visual workflow canvas**; multi-model support (OpenAI, local, etc.); built-in RAG pipeline (doc ingestion to QA); **agent** support with 50+ tools[github.com](https://github.com/langgenius/dify#:~:text=5,DALL·E%2C Stable Diffusion and WolframAlpha); monitoring & user management; deployable on-prem or cloud. |
| **Eino**              | Framework (open-source, Apache-2.0)  | Go                              | ~5k[github.com](https://github.com/cloudwego/eino#:~:text=5,Tags   Activity) | Strongly-typed orchestration in Go; chain and graph execution models; suitable for high-performance services; offers concurrency and streaming support; used internally at ByteDance (proven at scale). |
| **Promptulate**       | Library (open-source)                | Python                          | <1k                                                          | Lightweight agent framework (community-driven); easier integration with Chinese LLMs (like ERNIE); provides simple tool-use and automation patterns for local deployments. |
| **BaiLian (Alibaba)** | Cloud platform (proprietary)         | (Various)                       | –                                                            | Enterprise development suite on Alibaba Cloud; supports Alibaba’s Tongyi models; no-code interface for prompt and workflow design; tightly integrated with cloud services and security features. |
| **Baidu Qianfan**     | Cloud platform (proprietary)         | (Various)                       | –                                                            | One-stop model studio by Baidu; fine-tuning, prompt design, and deployment tools; supports ERNIE Bot and others; targeted at enterprise and government AI adoption with compliance. |

*(Table legend: “Low-code platform” indicates a tool with a GUI for building apps. **Stars** are approximate as of 2025. Proprietary cloud platforms don’t rely on GitHub star metrics. RAG = Retrieval-Augmented Generation.)*

## Understanding Workflow Structure

In the context of AI agents, a **workflow’s structure** refers to the organized sequence of steps or modules the agent follows to achieve its goal. In simpler terms, it is the “game plan” or blueprint of how the agent operates. An **agentic workflow** is typically composed of a series of connected actions (think of steps like *plan → act → observe → repeat*) that the agent executes, possibly in a loop, until the task is complete[weaviate.io](https://weaviate.io/blog/what-are-agentic-workflows#:~:text=An agentic workflow is a,evolving processes). Key elements of an agent’s workflow structure often include:

- **Task Decomposition and Planning:** The agent breaks down a complex goal into smaller subtasks. It may *“think”* or use the LLM to generate a plan or sequence of actions[weaviate.io](https://weaviate.io/blog/what-are-agentic-workflows#:~:text=,until the outcome is satisfactory). This is sometimes an explicit step (using a Planner) or implicit in the agent’s prompt strategy.
- **Tool Usage Actions:** The agent invokes external tools or APIs as needed. Each action typically involves the LLM deciding which tool to use and with what input (e.g. a search query, a calculator, database access, etc.), then executing that tool and receiving the result[blog.dataiku.com](https://blog.dataiku.com/open-source-frameworks-for-llm-powered-agents#:~:text=interact with their environment,and easily create customized tools)[weaviate.io](https://weaviate.io/blog/what-are-agentic-workflows#:~:text=). These action steps are the *“structured steps”* that make up the workflow.
- **Observation and Reflection:** After each tool use or intermediate step, the agent observes the result and evaluates progress. The workflow structure may include conditional logic or loops – for example, the agent checks if the goal is achieved or if another step is required. If not done, it feeds the new information back into the LLM for the next reasoning cycle[blog.langchain.com](https://blog.langchain.com/how-to-think-about-agent-frameworks/#:~:text=,The tools to use)[weaviate.io](https://weaviate.io/blog/what-are-agentic-workflows#:~:text=,until the outcome is satisfactory).
- **Termination/Output:** The workflow defines a condition or step where the agent should produce a final answer (and stop calling tools). This could be when the agent’s LLM decides to output an answer rather than an action, or when a preset number of iterations is reached[blog.langchain.com](https://blog.langchain.com/how-to-think-about-agent-frameworks/#:~:text=). The structure may include a formatting step to present the final result.

**Workflow vs. Agent:** It’s important to note the distinction between a rigid *workflow* and a more flexible *agent*. A **workflow** (in Anthropic’s terms) is a system of LLM calls and tool uses orchestrated through a predetermined, fixed sequence of steps (i.e. an LLM-powered pipeline)[anthropic.com](https://www.anthropic.com/engineering/building-effective-agents#:~:text=architectural distinction between workflows and,agents). An **agent**, by contrast, has a more dynamic structure – the LLM decides which steps to take and in what order, based on context, rather than following a hard-coded path[anthropic.com](https://www.anthropic.com/engineering/building-effective-agents#:~:text=architectural distinction between workflows and,agents). In practice, many real-world AI systems blend both: some parts are **structured workflows** (for reliability) and other parts are **agentic** (for flexibility)[blog.langchain.com](https://blog.langchain.com/how-to-think-about-agent-frameworks/#:~:text=In practice%2C we see that,way of thinking about things). The workflow’s structure thus refers to this overall architecture: how the deterministic parts (if any) and the agentic (dynamic) parts are organized into a sequence of operations.

In summary, the structure of an agent workflow is the *series of structured steps and decisions* that the agent will iterate through to reach a solution[zhuanlan.zhihu.com](https://zhuanlan.zhihu.com/p/1887650868763026084#:~:text=一文详细了解代理工作流(Agentic Workflows) ,Workflow）指的是代理为完成特定任务（最终目标）所采取的一系列结构化步骤。因此，当讨论代理工作流时，实际上是在讨论代理)[weaviate.io](https://weaviate.io/blog/what-are-agentic-workflows#:~:text=An agentic workflow is a,evolving processes). This could be as simple as a fixed chain of prompts (for a well-defined task), or as complex as a loop where the agent plans, uses a tool, and re-plans until a termination condition is met. A well-designed workflow structure clearly defines how the agent transitions from one step to the next, ensuring the process is coherent and can be inspected or controlled as needed.

## Clear Instructions and Output Formatting

No matter which framework is used, a fundamental best practice in LLM applications is to craft **clear prompts with explicit instructions**, especially regarding the desired output format. By default, LLMs output free-form text, which can be unpredictable for downstream use, so developers often need to guide the model to produce answers in a structured way. This section covers techniques and tools for ensuring LLM outputs are well-formatted and compliant with specifications.

**1. Prompt Clarity:** Always specify in the prompt *what format* you expect the answer in. For example, if a JSON output is needed, the prompt might say: *“Provide the answer as a JSON object with keys `X`, `Y`, `Z`.”* If the user expects a bullet list, the prompt should mention “answer in bullet points”. Many frameworks provide convenient prompt templates for this. For instance, LangChain and others let you define an output parser or schema and will append instructions like *“You should only reply with valid JSON.”* Explicit instructions significantly raise the chance the model formats its response correctly[medium.com](https://medium.com/data-science/safeguarding-llms-with-guardrails-4f5d9f57cff2#:~:text=the AI model is operating,defined principles in an organization)[medium.com](https://medium.com/data-science/safeguarding-llms-with-guardrails-4f5d9f57cff2#:~:text=The goal of guardrails is,and quality of LLM responses). Using few-shot examples can further reinforce the format (e.g., showing a sample input and the perfectly formatted output).

**2. Structured Output Parsers:** Several libraries help validate and coerce LLM outputs into a desired structure:

- **Guardrails AI (Python):** Guardrails allows developers to define an output schema and validation rules in a special XML-based .rail file, and wraps the LLM call to enforce these rules[medium.com](https://medium.com/data-science/safeguarding-llms-with-guardrails-4f5d9f57cff2#:~:text=Guardrails AI is an open,enforce structure and type guarantees)[medium.com](https://medium.com/data-science/safeguarding-llms-with-guardrails-4f5d9f57cff2#:~:text=Guardrails is built on RAIL,is the core of guardrails). Under the hood, it will check if the LLM’s response conforms to the schema (for example, valid JSON with certain fields, or text without disallowed content) and can automatically trigger a retry or corrective prompt if not[medium.com](https://medium.com/data-science/safeguarding-llms-with-guardrails-4f5d9f57cff2#:~:text=1,sent to an LLM application)[medium.com](https://medium.com/data-science/safeguarding-llms-with-guardrails-4f5d9f57cff2#:~:text=docs github,language description of the problem). Guardrails essentially provides a layer of assurance – using Pydantic-like type checking and even semantic checks – on top of the LLM. This way, you can guarantee the final output *always* meets your format (or the system keeps asking until it does or aborts). It’s especially useful for enforcing structured outputs (JSON, SQL queries, etc.) and for injecting safety filters. NVIDIA’s NeMo Guardrails is a similar concept, aimed at enterprise deployments.
- **Zod + TypeChat (TypeScript):** In the Node.js/TypeScript ecosystem, a common approach is to use the Zod library to define the expected schema of the LLM output, and pair it with techniques to validate the model response. **Zod** is a popular TS library for schema validation which can be used to both describe the output format and validate data at runtime[medium.com](https://medium.com/@canadaduane/using-zod-to-build-structured-chatgpt-queries-c3c7f4d55d92#:~:text=Here’s a pattern I’ve found,back%2C validating what it returns). Microsoft’s experimental **TypeChat** library builds on this: you define TypeScript interfaces or Zod schemas for the response, and TypeChat will format a system prompt that includes those type definitions (converting them to a readable form) so that the LLM is instructed to follow that structure[medium.com](https://medium.com/@canadaduane/using-zod-to-build-structured-chatgpt-queries-c3c7f4d55d92#:~:text=The pattern begins with Javascript’s,that checks and validates types)[medium.com](https://medium.com/@canadaduane/using-zod-to-build-structured-chatgpt-queries-c3c7f4d55d92#:~:text=Using this ,ts). After the LLM responds, the library parses the JSON and uses Zod to check types – if validation fails, it can automatically generate a follow-up prompt like “Repair the output to match the schema” and send it to the model[medium.com](https://medium.com/@canadaduane/using-zod-to-build-structured-chatgpt-queries-c3c7f4d55d92#:~:text=Here’s how we send the,request and parse the response). This loop continues until the output is valid or a limit is reached. By leveraging Zod schemas, developers get a clear contract for the AI and can handle errors systematically. The Medium snippet below illustrates this pattern, where a `HorseSchema` is defined in Zod, included in the prompt (with each field described), and then used to parse the result into a typed object[medium.com](https://medium.com/@canadaduane/using-zod-to-build-structured-chatgpt-queries-c3c7f4d55d92#:~:text=export const HorseSchema %3D z.object(,of the horse in years)[medium.com](https://medium.com/@canadaduane/using-zod-to-build-structured-chatgpt-queries-c3c7f4d55d92#:~:text=const response %3D await openai.createChatCompletion(,content }]):

> *“Here’s a Zod schema... you can create a run-time schema that checks and validates types. It turns out, you can also describe fields, and this is handy when building queries to send to OpenAI’s API... The request ... says: Respond only as a JSON document, and strictly conform to the following TypeScript schema... (After getting the response) if it’s a string, we do `HorseSchema.parse(JSON.parse(responseContent))`.”*[medium.com](https://medium.com/@canadaduane/using-zod-to-build-structured-chatgpt-queries-c3c7f4d55d92#:~:text=The pattern begins with Javascript’s,that checks and validates types)[medium.com](https://medium.com/@canadaduane/using-zod-to-build-structured-chatgpt-queries-c3c7f4d55d92#:~:text=model%3A "gpt,content }])

This approach yields a well-structured JSON object in code or an error if the model didn’t comply, which you can then handle (e.g., ask again or throw). The benefit of TypeChat/Zod is that it aligns with developers’ normal practice of defining types, bringing LLM output into the traditional type-checked workflow.

- **Pydantic & Others (Python):** Similar to Zod for TS, Python developers often use **Pydantic** data models to define the shape of an LLM response. For example, one might create a Pydantic class with fields `answer: str` and `source_urls: List[str]`, and then prompt the LLM to output a JSON for that. After the LLM responds, one can do `MyModel.parse_raw(llm_output)` to get a Python object or an error. Libraries like LangChain have `StructuredOutputParser` that works with Pydantic to simplify this pattern (it automatically generates the few-shot prompt with the schema and does the parsing). Marvin, as mentioned, embraces this by letting you return Pydantic objects from AI functions – it will internally loop until parsing succeeds. The principle across these is the same: by formalizing the expected output structure, we both guide the LLM via the prompt and verify the result programmatically.

**3. Model-Specific Features:** Newer model APIs provide native ways to get structured output:

- **OpenAI Function Calling & JSON Mode:** OpenAI’s API (for GPT-3.5-Turbo and GPT-4) introduced *function calling* in mid-2023, and later an explicit *“structured output” mode in 2024*. With function calling, you define functions with JSON schema parameters, and the model can directly output a JSON object conforming to that schema (the API returns a structured Python dict, not just text) if you set `function_call` in the prompt. In 2024, OpenAI added a `strict`: true option for function definitions which forces the model to adhere exactly to the provided JSON schema[openai.com](https://openai.com/index/introducing-structured-outputs-in-the-api/#:~:text=We’re introducing Structured Outputs in,two forms in the API)[openai.com](https://openai.com/index/introducing-structured-outputs-in-the-api/#:~:text=We are introducing Structured Outputs,supplied JSON Schemas). This dramatically improves reliability – OpenAI reported their GPT-4 with Structured Outputs (strict) achieved *100% compliance with complex schemas*, versus <40% with prompt-based attempts[openai.com](https://openai.com/index/introducing-structured-outputs-in-the-api/#:~:text=needed to interoperate with their,to better understand complicated schemas)[openai.com](https://openai.com/index/introducing-structured-outputs-in-the-api/#:~:text=On our evals of complex,0613` scores less than 40). Essentially, the model’s decoder is constrained so it can only emit tokens that form valid JSON per the schema. This is a game-changer for tasks like form-filling, code generation, or any case where a strict format is needed, as it removes the need for multiple validation retries. Whenever available, using such native features is recommended because they enforce correctness at the model level. For instance, to get a list of items with name and price, you can specify a function `submit_order(items: list[Item])` where `Item` has `name` and `price` fields; if `strict: true`, the model will only respond with `{"items": [...]} ` matching the exact types, or trigger an error internally if it can’t.
- **Other Providers:** Competing LLM services also offer similar capabilities – e.g., Anthropic’s Claude has a “Constitutional AI” approach to follow formatting instructions diligently, and some open-source libraries like **Transformers** allow you to apply **output regex constraints** or **logit processors** to bias the model toward certain formats (though these are advanced uses and typically need self-hosting a model). For example, you could use a regex that matches a JSON and have the generator constrained to that pattern[boundaryml.com](https://www.boundaryml.com/blog/structured-output-from-llms#:~:text=LLMs make a lot of,help you handle these errors)[boundaryml.com](https://www.boundaryml.com/blog/structured-output-from-llms#:~:text=There are two techniques that,and a few others do), though this can be non-trivial for complex outputs.

**4. Instruction Styling:** When writing system or user prompts for formatting, be as direct and specific as possible. Good practices include:

- Stating the format in **capital letters** or special tokens (some use `<json>` tags or markdown to signal the model). E.g., *“OUTPUT ONLY THE JSON, NOTHING ELSE.”* or *“Begin your answer with `BEGINJSON` and end with `ENDJSON`.”* These help the model avoid extraneous text.
- If using certain frameworks, leverage their built-in format enforcers. (LangChain’s `OutputFixingParser` will attempt to repair a JSON by parsing error messages, for instance. Guardrails, as mentioned, can auto-correct or re-ask.)
- Provide an example of a well-formatted output after a sample query in the prompt (few-shot). Models often learn format better from examples than from description alone.
- Reminding the model about the format if the dialogue is long (you can have the system prompt always include the instruction to answer in JSON).

In summary, **clear instructions and output schemas** are critical for turning the LLM from a creative text generator into a reliable component of a software system. Modern frameworks and libraries are increasingly incorporating these techniques so that developers can **treat LLM outputs like typed function returns or API responses**, which is essential for building complex applications with high correctness requirements. By combining strong prompting practices with validation tools (like Guardrails or Zod/TypeChat) and taking advantage of features like OpenAI’s structured output, one can minimize the “hallucinated formatting” issue and ensure the AI’s responses can be consumed directly by programs down the line[medium.com](https://medium.com/data-science/safeguarding-llms-with-guardrails-4f5d9f57cff2#:~:text=The goal of guardrails is,and quality of LLM responses)[medium.com](https://medium.com/@canadaduane/using-zod-to-build-structured-chatgpt-queries-c3c7f4d55d92#:~:text=model%3A "gpt,content }]).

## Content Safety Strategy During Agent Execution

Content safety is a critical aspect of deploying AI agents, especially as they operate autonomously and interact with varied inputs. An **agent workflow must incorporate safeguards** to prevent harmful, sensitive, or policy-violating content from being produced at any step. Formulating a content safety strategy involves multiple layers:

- **System-Level Guidelines (Policy Prompts):** Set the tone with system instructions that outline unacceptable content. For example, a system message to the LLM might include: *“The assistant must not produce profanity, hate speech, or personal identifiable information.”* Clear policies at the prompt level help align the model’s behavior from the start[medium.com](https://medium.com/@kbdhunga/guardrails-in-llm-applications-ensuring-safe-and-effective-ai-use-5b7d09d80dff#:~:text=Guardrails in llm applications are,technical%2C ethical%2C or contextual boundaries)[altexsoft.com](https://www.altexsoft.com/blog/ai-guardrails/#:~:text=AI guardrails are controls and,harmful%2C incorrect%2C or unintended behavior). Many LLMs (especially those fine-tuned with Reinforcement Learning from Human Feedback) have built-in compliance if instructed properly to follow a content policy.
- **Input and Output Filtering:** Implement filters on both user inputs and the agent’s outputs. **Moderation APIs** or models can scan the text for disallowed content. For instance, OpenAI’s Moderation API or similar services can be called on the user query before the agent sees it, and on the agent’s response before it’s returned to the user. If something flagged appears (e.g. sexual content, violence, politically sensitive terms), the system can either refuse that request or sanitize the output. This acts as a safety net to catch content that the model’s internal guardrails might miss[altexsoft.com](https://www.altexsoft.com/blog/ai-guardrails/#:~:text=Have you ever asked an,systems can and can’t do)[altexsoft.com](https://www.altexsoft.com/blog/ai-guardrails/#:~:text=Fighting h allucinations and misinformation,false claims and inaccurate information).
- **Tool Usage Safeguards:** When agents use tools (like running code, web searches, or APIs), impose restrictions to prevent unsafe actions. This can be done by **sandboxing** certain operations and limiting permissions. For example, if the agent has a tool to execute code, run it in a sandboxed environment with no network access or file write permissions to avoid harm. If the agent can query a database, give it read-only credentials so it cannot modify data. These are *“in-tool guardrails”* – designing tools such that only safe operations are allowed[adk.wiki](https://adk.wiki/safety/#:~:text=,沙盒化代码执行： 通过对环境进行沙盒化，防止模型生成的代码导致安全问题。)[adk.wiki](https://adk.wiki/safety/#:~:text=,沙盒化代码执行： 通过对环境进行沙盒化，防止模型生成的代码导致安全问题。). Developers can also hard-code constraints in tool wrappers (e.g. strip out any disallowed command in a shell tool, or block certain websites in a web search tool).
- **Callbacks and Intervention Points:** Many frameworks support **callbacks** or hooks at various stages of the agent’s loop. You can use these to inject safety checks. For example, after each LLM response (which may contain a tool decision or message), run a function that examines the content for policy violations or checks if the intended tool use is allowed. If something is amiss (say the agent is about to output a confidential secret or use a forbidden tool), the callback can modify the agent’s action or halt the workflow[adk.wiki](https://adk.wiki/safety/#:~:text=,沙盒化代码执行： 通过对环境进行沙盒化，防止模型生成的代码导致安全问题。). This design allows dynamic intervention: the agent’s behavior is monitored in real-time, not just at input/output boundaries.
- **Multi-Stage Moderation (Assistant Agents):** A more advanced strategy is to employ a secondary **“moderator” agent or model** in the workflow. For instance, one could have the primary agent generate a draft answer, then a second agent (or a classifier model) reviews that draft for safety and either approves it or instructs the primary agent to revise it if it violates guidelines. This resembles a human-in-the-loop, except automated. Anthropic’s *parallel guardrails* pattern notes that using a separate model instance to screen content can be more effective than having the same model try to self-censor in-line[anthropic.com](https://www.anthropic.com/engineering/building-effective-agents#:~:text=Examples where parallelization is useful%3A).
- **Continuous Updates and Compliance:** Keep the safety strategy updated with the latest policies and regulations. This is particularly important in different jurisdictions. For example, in the US and Europe, privacy and fairness might be key concerns, whereas in China the law imposes additional strict rules on politically sensitive content, rumors, etc. Chinese AI platforms typically have *pre-defined keyword blacklists and compliance checks* – e.g., Huawei’s XiaoYi platform guidelines forbid agents from any content that violates laws, promotes illegal activity, or contains pornography/vulgarity[developer.huawei.com](https://developer.huawei.com/consumer/cn/doc/service/content-security-compliance-requirements-0000002305029197#:~:text=内容安全合规要求). Ensure your agent’s safety filters include such locale-specific rules as needed. Ultimately, the **brand and user safety** considerations should drive what your agent is never allowed to say or do, and those should be encoded in multiple layers of defense (prompt instructions, filtered tools, moderated I/O, etc.)[altexsoft.com](https://www.altexsoft.com/blog/ai-guardrails/#:~:text=Excluding bias and harmful narratives,and doesn’t reinforce harmful narratives)[altexsoft.com](https://www.altexsoft.com/blog/ai-guardrails/#:~:text=Privacy and data protection,requests that seek private information).

A robust content safety strategy uses **redundancy**: multiple guardrails working together to catch different failure modes. As an example, OpenAI’s reference architecture for agents combines LLM-based content filters, rule-based filters (like regex checks for sensitive data), and the model’s own refusals to ensure compliance at various points[altexsoft.com](https://www.altexsoft.com/blog/ai-guardrails/#:~:text=Guardrails are essential in the,AI stays within acceptable limits)[altexsoft.com](https://www.altexsoft.com/blog/ai-guardrails/#:~:text=Abuse prevention,prompts the LLM can process). By planning for safety from the outset and using the tools above, you can significantly reduce the risk of an agent going off the rails. Remember that agents can affect the real world (by tool actions or by influencing users), so **safety is not just about avoiding bad text** – it’s also about preventing actions beyond the agent’s scope. Guardrails define the *boundaries* of the agent’s autonomy[altexsoft.com](https://www.altexsoft.com/blog/ai-guardrails/#:~:text=Guardrails are needed—and arguably more,processes%2C not just produce text)[altexsoft.com](https://www.altexsoft.com/blog/ai-guardrails/#:~:text=,to irrelevant or harmful requests), ensuring it stays within acceptable limits both in content and in behavior.

## Tracing and Observability of Agent Workflows

**Tracing** refers to logging and monitoring the step-by-step execution of an agent’s workflow. Given the complexity and non-deterministic nature of LLM agents, it is essential to have good observability into what the agent is doing at each stage. Effective tracing allows developers to debug issues, optimize performance, and ensure the agent’s reasoning aligns with expectations[langchain.com](https://www.langchain.com/langsmith#:~:text=Find failures fast with agent,observability)[langchain.com](https://www.langchain.com/langsmith#:~:text=LangSmith traces contain the full,Learn more).

Key aspects of tracing an agent include recording:

- **Prompts and Model Responses:** Every time the agent calls the LLM (for a decision or to produce output), the exact prompt and the LLM’s response should be captured. This includes system messages, user inputs, and the assistant’s reply at each turn. By inspecting these, one can understand *why* the agent took a certain action or produced a certain answer.
- **Tool Invocations:** Each tool usage (what tool was called and with what input) and the result returned by the tool should be logged. This is crucial to see if a tool is used correctly and what data it provided to the agent.
- **Intermediate State/Memory:** If the agent maintains any memory or intermediate state (e.g. a list of tasks in a todo list agent, or a chain-of-thought in hidden prompts), tracing should capture the updates to this state over time.
- **Final Output and Termination Reason:** Log the final answer given by the agent and note why the agent stopped (did it reach a stopping criterion, run out of steps, or encounter an error?).

Modern agent frameworks often come with built-in tracing or easy integration to observability tools. For example, **LangChain’s LangSmith** platform provides detailed traces containing all inputs and outputs of each step, giving developers full visibility into their agent’s behavior[langchain.com](https://www.langchain.com/langsmith#:~:text=LangSmith traces contain the full,Learn more). This means you can inspect a trace and see the entire sequence of LLM calls, decisions, and tool actions that led to the final answer. Likewise, the OpenAI Agents SDK automatically **traces agent runs** and can send those traces to external monitoring services (like Logfire, AgentOps, etc.) via an OpenTelemetry-compatible interface[github.com](https://github.com/openai/openai-agents-python#:~:text=The Agents SDK automatically traces,list of external tracing processors). These tools often provide a web dashboard or logs where each agent “run” is indexed, and you can drill down into each step of the run.

To implement tracing in your own agent (if not using an off-the-shelf solution), you can insert logging statements at critical points in the code. Many developers use simple logging to console or files for each step when prototyping. For instance, after the LLM returns a message, print out the message; when a tool is called, print the tool name and parameters; when a tool returns, print the result. Over time, however, a more structured approach is beneficial: using an observability SDK or a database to store traces can enable searching and aggregating across many runs. This helps in identifying common failure patterns. Tracing is also invaluable for **performance monitoring** – you can log the time each step takes, or the tokens used, and use that to optimize the workflow (e.g. if a certain tool call is very slow or a prompt is too long).

In summary, **tracing provides the transparency needed to trust and refine an agentic system**. With full trace logs, you can answer questions like: *Why did the agent take that action? Where did the wrong answer creep in? How many steps did it use?* Armed with those insights, you can iteratively improve the agent’s prompts or logic. It’s often said that with LLM applications, *observability is key*, because traditional unit tests may not catch the unpredictable behaviors – but a trace will show you exactly what happened in each instance[langchain.com](https://www.langchain.com/langsmith#:~:text=agent or LLM app performance,Learn more)[langchain.com](https://www.langchain.com/langsmith#:~:text=Where is LangSmith data stored%3F). Therefore, make tracing (and reviewing traces) a regular part of your agent development cycle.